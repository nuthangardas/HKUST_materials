{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.7, 2.5, 5. , 2. ],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [5.9, 3. , 5.1, 1.8],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.1, 3.5, 1.4, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [5.8, 2.7, 5.1, 1.9]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data\n",
    "data=np.load(\"/Users/gardasnagarjun/Downloads/datasets/bi-class/iris.npz\", mmap_mode='r')\n",
    "X_train=data['train_X'].astype('float32')\n",
    "train_Y=data['train_Y'].astype('long')\n",
    "X_test,test_Y=data['test_X'].astype('float32'),data['test_Y'].astype('long')\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# splitting training dataset into train and validation dataset:\n",
    "train_X,val_X,train_y,val_Y=train_test_split(X_train,train_Y,test_size=0.2)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 4) (24, 4) (96,) (24,)\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the datasets:\n",
    "print(train_X.shape,val_X.shape,train_y.shape,val_Y.shape)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "#Construct main,train,validation and test datasets for torch.util.data.dataloader:\n",
    "\n",
    "main_data=[]\n",
    "for i in range(len(X_train)):\n",
    "    X_train.astype('float32')\n",
    "    train_Y.astype('long')\n",
    "    main_data.append([X_train[i],train_Y[i]])\n",
    "    \n",
    "mainloader=torch.utils.data.DataLoader(main_data,shuffle=True,batch_size=1)\n",
    "i4,l4=next(iter(mainloader))\n",
    "print(i4.shape)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(train_X)):\n",
    "    train_X.astype('float32')\n",
    "    train_y.astype('long')\n",
    "    train_data.append([train_X[i], train_y[i]])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=1)\n",
    "i1, l1 = next(iter(trainloader))\n",
    "print(i1.shape)\n",
    "\n",
    "val_data=[]\n",
    "for i in range(len(val_X)):\n",
    "    val_X.astype('float32')\n",
    "    val_Y.astype('long')\n",
    "    val_data.append([val_X[i], val_Y[i]])\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(val_data, shuffle=False, batch_size=1)\n",
    "i2, l2 = next(iter(valloader))\n",
    "print(i2.shape)\n",
    "\n",
    "\n",
    "test_data=[]\n",
    "for i in range(len(X_test)):\n",
    "    X_test.astype('float32')\n",
    "    test_Y.astype('long')\n",
    "    test_data.append([X_test[i],test_Y[i]])\n",
    "    \n",
    "testloader=torch.utils.data.DataLoader(test_data,shuffle=False)\n",
    "\n",
    "i3,l3=next(iter(testloader))\n",
    "print(i3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "96\n",
      "24\n",
      "30\n",
      "0 [tensor([[6.4000, 2.9000, 4.3000, 1.3000]]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "print(len(mainloader))\n",
    "print(len(trainloader))\n",
    "print(len(valloader))\n",
    "print(len(testloader))\n",
    "for i,j in enumerate(valloader):\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model of your Neural Network:\n",
    "class One_hidden(nn.Module):\n",
    "    def __init__(self,n_hidden,n_output=2,n_feature=4):\n",
    "        super(One_hidden,self).__init__()\n",
    "        self.hidden=nn.Linear(n_feature,n_hidden)\n",
    "        self.output=nn.Linear(n_hidden,n_output)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.hidden(x))\n",
    "        x=self.output(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One_hidden(\n",
      "  (hidden): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (output): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_10=One_hidden(10)\n",
    "print(net_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with step parameter:\n",
    "\n",
    "def train_with_steps(net,trainloader):\n",
    "    criterion = nn.CrossEntropyLoss() # Loss function\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001) # optimizer\n",
    "    #torch.manual_seed(0)\n",
    "    n_total_steps = len(trainloader) # total steps\n",
    "    num_epochs=20\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i,(x,y) in enumerate(trainloader):\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # Forward pass    \n",
    "            output = net(x)\n",
    "            loss = criterion(output, y)\n",
    "        \n",
    "        \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            if (i+1) % 12 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}],Step:[{i+1}/{n_total_steps}], Loss: {loss.item():.5f}')\n",
    "                \n",
    "    acc=accuracy(net,valloader)\n",
    "    return acc\n",
    "            \n",
    " #Calculate Accuracy:      \n",
    "def accuracy(net,valloader):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        n_correct=0\n",
    "        n_samples=0\n",
    "        for x,y in valloader:\n",
    "            output=net(x)\n",
    "            _,predicted=torch.max(output.data,1)\n",
    "            n_samples+=y.size(0)\n",
    "            n_correct+=(predicted==y).sum().item()\n",
    "            \n",
    "        acc=100.0* n_correct/n_samples\n",
    "        print(f'Accuracy of the network for val data: {acc} %')\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(net,testloader):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for z, y in testloader:\n",
    "            outputs = net(z)\n",
    "            # max returns (value ,index)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            n_samples += y.size(0)\n",
    "            n_correct += (predicted == y).sum().item()\n",
    "            \n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network for test data: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20],Step:[12/96], Loss: 0.74092\n",
      "Epoch [1/20],Step:[24/96], Loss: 0.65890\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.62067\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.71236\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.57600\n",
      "Epoch [1/20],Step:[72/96], Loss: 0.87692\n",
      "Epoch [1/20],Step:[84/96], Loss: 0.65636\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.60303\n",
      "Epoch [2/20],Step:[12/96], Loss: 0.92452\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.81590\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.54661\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.71696\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.45132\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.83024\n",
      "Epoch [2/20],Step:[84/96], Loss: 0.34283\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.86980\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.43738\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.92754\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.30138\n",
      "Epoch [3/20],Step:[48/96], Loss: 1.08783\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.61264\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.27468\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.32924\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.82105\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.25248\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.37811\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.85277\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.39913\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.45188\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.29751\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.46535\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.34893\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.41829\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.21823\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.33385\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.34721\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.23097\n",
      "Epoch [5/20],Step:[72/96], Loss: 1.03961\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.21444\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.95406\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.74460\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.90809\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.25357\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.34502\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.72816\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.39804\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.71730\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.20351\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.84223\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.91258\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.78586\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.18422\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.92019\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.23354\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.24359\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.39000\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.24862\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.33261\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.69020\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.78146\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.33612\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.16481\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.35871\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.66327\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.21510\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.16095\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.29562\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.17961\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.29848\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.70086\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.16844\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.78393\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.09040\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.85775\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.71154\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.41250\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.23333\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.66844\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.09311\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.13302\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.63087\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.67000\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.13519\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.67552\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.51106\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.16409\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.19543\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.53188\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.61895\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.60730\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.11151\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.72047\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.27457\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.59470\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.16530\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.14148\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.18262\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.16908\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.29060\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.44915\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.73002\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.58114\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.25724\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.19102\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.12836\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.38137\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.07582\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.52231\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.09510\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.23305\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.24261\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.48016\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.42795\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.47661\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.08720\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.23666\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.53483\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.06204\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.46751\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.31900\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.19078\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.19205\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.10744\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.18005\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.32486\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.11958\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.42412\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.39848\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.31192\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.43047\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.10238\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.11470\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.38325\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.14742\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.50712\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.25661\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.14651\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.10179\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.10120\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.40643\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.19056\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.04068\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.28841\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.37688\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.38038\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.09911\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.05700\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.01467\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.13462\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.12790\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.39102\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.16096\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.12741\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.34805\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.04687\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.23143\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.03255\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.19396\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.32305\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.43918\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 1.36008\n",
      "Epoch [1/20],Step:[24/96], Loss: 1.25063\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.36885\n",
      "Epoch [1/20],Step:[48/96], Loss: 1.25460\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.32902\n",
      "Epoch [1/20],Step:[72/96], Loss: 1.26440\n",
      "Epoch [1/20],Step:[84/96], Loss: 0.35724\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.38147\n",
      "Epoch [2/20],Step:[12/96], Loss: 1.13516\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.44878\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.44371\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.46205\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.45676\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.41751\n",
      "Epoch [2/20],Step:[84/96], Loss: 0.37890\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.37758\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.33199\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.38552\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.30310\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.38403\n",
      "Epoch [3/20],Step:[60/96], Loss: 1.06696\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.36126\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.33806\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.37069\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.32791\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.29714\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.35884\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.99235\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.32974\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.29393\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.93142\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.25677\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.92089\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.89411\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.90506\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.92462\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.21805\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.37215\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.85787\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.95178\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.19532\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.75329\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.82736\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.74073\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.70299\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.78480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20],Step:[84/96], Loss: 0.32587\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.34935\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.14442\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.66060\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.18399\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.21862\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.62531\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.20661\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.69950\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.22202\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.29274\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.11006\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.16286\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.60038\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.57181\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.17407\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.14712\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.09621\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.15180\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.27881\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.42317\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.10490\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.17446\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.33408\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.36095\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.25481\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.39593\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.32527\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.31487\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.27278\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.08172\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.22887\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.33429\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.27598\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.06214\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.03380\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.30029\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.06358\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.14389\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.28000\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.03419\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.14542\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.13958\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.17231\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.18188\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.14356\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.14636\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.07251\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.17428\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.20972\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.10182\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.05074\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.16155\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.29784\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.15557\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.12191\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.21705\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.20688\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.11445\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.11964\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.08135\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.16260\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.01402\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.05054\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.03783\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.01213\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.02727\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.13070\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.15323\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.05356\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.15816\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.01700\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.16660\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.10953\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.07926\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.12052\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.00975\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.13547\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.01062\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.08269\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.01419\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.01448\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.12234\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.08563\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.02027\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.07148\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.01951\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.10045\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.07769\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.01782\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.00659\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.00613\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.05221\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.09140\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.03409\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.07426\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.09385\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.09563\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.02172\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.03096\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.06994\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.04867\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.00385\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.05684\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.01042\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00843\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.01402\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.01353\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.03549\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.01139\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.04143\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.03825\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.03099\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.00262\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 2.35884\n",
      "Epoch [1/20],Step:[24/96], Loss: 2.12096\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.13416\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.14208\n",
      "Epoch [1/20],Step:[60/96], Loss: 1.93039\n",
      "Epoch [1/20],Step:[72/96], Loss: 1.67157\n",
      "Epoch [1/20],Step:[84/96], Loss: 1.50660\n",
      "Epoch [1/20],Step:[96/96], Loss: 1.40809\n",
      "Epoch [2/20],Step:[12/96], Loss: 1.38602\n",
      "Epoch [2/20],Step:[24/96], Loss: 1.28479\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.26336\n",
      "Epoch [2/20],Step:[48/96], Loss: 1.11445\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.31128\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.27002\n",
      "Epoch [2/20],Step:[84/96], Loss: 1.01981\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.30138\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.26717\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.91134\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.86602\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.32335\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.81414\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.79848\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.73532\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.71747\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.82384\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.74207\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.66220\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.64427\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.62339\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.31665\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.33387\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.53629\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.54691\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.49873\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.51994\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.44700\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.44840\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.42340\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.42734\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.38790\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.41281\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.47432\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.32748\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.30327\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.32549\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.49905\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.34448\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.35015\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.34938\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.34357\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.21773\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.18566\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.18086\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.36255\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.17097\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.32022\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.26156\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.11348\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.22918\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.25496\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.25792\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.33765\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.14036\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.16816\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.22582\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.22661\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.26697\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.23749\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.08442\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.09123\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.11304\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.06827\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.17312\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.16298\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.12779\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.29114\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.22827\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.23100\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.06371\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.09976\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.13768\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.07734\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.20542\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.13651\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.23585\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.04686\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.04060\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.17356\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.10983\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.04392\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.21971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20],Step:[48/96], Loss: 0.03067\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.03422\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.12385\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.08020\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.12828\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.16581\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.03617\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.03315\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.06243\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.12660\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.06458\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.02720\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.08645\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.09588\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.11446\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.10046\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.09425\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.10502\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.01673\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.12344\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.10321\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.12290\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.04023\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.06585\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.09827\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.02677\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.05590\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.02722\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.07633\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.02402\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.07094\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.05729\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.02337\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.05049\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.04927\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.00803\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.02204\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.00680\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.04700\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.01124\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.05083\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.03732\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.00887\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.06497\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.06001\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.04753\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.05733\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.00959\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.07204\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.04580\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.04769\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.01152\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.02931\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.00941\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.02863\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.00477\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.01378\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.05284\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.06139\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.04165\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00996\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.00638\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.01026\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.04854\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.01250\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.00723\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.01949\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.03274\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.03392\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 1.29674\n",
      "Epoch [1/20],Step:[24/96], Loss: 0.31668\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.26422\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.29734\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.25658\n",
      "Epoch [1/20],Step:[72/96], Loss: 0.28229\n",
      "Epoch [1/20],Step:[84/96], Loss: 1.24538\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.30711\n",
      "Epoch [2/20],Step:[12/96], Loss: 0.22048\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.31902\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.23361\n",
      "Epoch [2/20],Step:[48/96], Loss: 1.24138\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.23087\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.29046\n",
      "Epoch [2/20],Step:[84/96], Loss: 1.15285\n",
      "Epoch [2/20],Step:[96/96], Loss: 1.11972\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.27318\n",
      "Epoch [3/20],Step:[24/96], Loss: 1.08631\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.25068\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.22514\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.31840\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.28582\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.23212\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.23996\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.30608\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.23920\n",
      "Epoch [4/20],Step:[36/96], Loss: 1.09123\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.28658\n",
      "Epoch [4/20],Step:[60/96], Loss: 1.06571\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.25430\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.28923\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.87763\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.32427\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.91384\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.29844\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.19083\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.95599\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.86494\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.99085\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.85944\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.17674\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.31309\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.18032\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.89416\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.90249\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.27175\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.18329\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.16876\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.13591\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.31118\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.87607\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.87511\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.09489\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.24333\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.13812\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.17907\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.70067\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.76421\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.77113\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.66062\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.16194\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.08490\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.65879\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.16778\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.64232\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.11606\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.62808\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.18765\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.71660\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.02275\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.07321\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.14429\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.04561\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.03191\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.60211\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.06452\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.59334\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.11071\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.02162\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.01920\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.04035\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.01815\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.08788\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.56993\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.10883\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.00649\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.55724\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.55259\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.60793\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.07635\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.01983\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.53799\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.08447\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.56283\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.00852\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.52311\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.51939\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.08141\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.51042\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.01223\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.00868\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.50261\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.07210\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.00209\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.51015\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.00254\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.12504\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.01465\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.04051\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.05635\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.47027\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.46704\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.02516\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.00760\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.03158\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.04515\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.01236\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.05235\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.44905\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.00138\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.04192\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.03122\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.45649\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.42672\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.00195\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.03103\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.00291\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.00035\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.41278\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.40868\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.00528\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.00301\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.05268\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.00877\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.03479\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.00265\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.04352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20],Step:[24/96], Loss: 0.03023\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.42100\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.00136\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.37946\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.37620\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.05601\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.37019\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.00663\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.02366\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.00283\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.03365\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.00886\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.00725\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.35304\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.01772\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.01779\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.00513\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.00114\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.00160\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.00118\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.00964\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.33202\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.07966\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 0.96450\n",
      "Epoch [1/20],Step:[24/96], Loss: 0.99130\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.84163\n",
      "Epoch [1/20],Step:[48/96], Loss: 1.07066\n",
      "Epoch [1/20],Step:[60/96], Loss: 1.06211\n",
      "Epoch [1/20],Step:[72/96], Loss: 1.12985\n",
      "Epoch [1/20],Step:[84/96], Loss: 0.57326\n",
      "Epoch [1/20],Step:[96/96], Loss: 1.12231\n",
      "Epoch [2/20],Step:[12/96], Loss: 1.08398\n",
      "Epoch [2/20],Step:[24/96], Loss: 1.11956\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.41268\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.39448\n",
      "Epoch [2/20],Step:[60/96], Loss: 1.18520\n",
      "Epoch [2/20],Step:[72/96], Loss: 1.07490\n",
      "Epoch [2/20],Step:[84/96], Loss: 1.07516\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.38913\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.36712\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.39792\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.93380\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.32067\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.92446\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.32567\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.39238\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.38872\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.34232\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.41748\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.32053\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.76565\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.24001\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.74012\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.74929\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.34229\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.26315\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.20593\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.24860\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.66377\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.62402\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.56138\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.39486\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.53115\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.58461\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.15974\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.55173\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.12822\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.26618\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.09715\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.18422\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.27675\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.28952\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.11326\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.54319\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.40574\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.09200\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.45003\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.32762\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.34420\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.10751\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.37787\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.24768\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.25735\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.26450\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.14689\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.18220\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.05323\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.29175\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.11881\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.07968\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.16039\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.05245\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.22511\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.15831\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.07618\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.17228\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.09315\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.26702\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.06680\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.03936\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.19952\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.03182\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.17294\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.12801\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.14051\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.18605\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.16483\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.19640\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.09420\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.03884\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.20287\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.16874\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.01326\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.13579\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.08763\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.09665\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.12372\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.04787\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.17408\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.14130\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.02260\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.14696\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.09044\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.05578\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.00946\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.01722\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.09191\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.08617\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.07002\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.08760\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.00874\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.03762\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.11127\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.11478\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.13993\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.09822\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.02068\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.01961\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.00698\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.04665\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.11799\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.09015\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.10005\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.04923\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.06265\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.01603\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.07575\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.01669\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.07091\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.05879\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.05167\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.06303\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.03345\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.04147\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.02389\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.00619\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.00482\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.05146\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.03878\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.08541\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.05825\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.02017\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.05067\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.07654\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.07415\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.01295\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.00121\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.00354\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.05583\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.00108\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.02607\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.03230\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.00388\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.00160\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00366\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.00358\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.06915\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.03798\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.00138\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.05445\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.00671\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.04615\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.08474\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 1.62989\n",
      "Epoch [1/20],Step:[24/96], Loss: 1.52953\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.35175\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.26637\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.22069\n",
      "Epoch [1/20],Step:[72/96], Loss: 0.24599\n",
      "Epoch [1/20],Step:[84/96], Loss: 1.23068\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.23988\n",
      "Epoch [2/20],Step:[12/96], Loss: 1.32532\n",
      "Epoch [2/20],Step:[24/96], Loss: 1.26209\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.33419\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.30062\n",
      "Epoch [2/20],Step:[60/96], Loss: 1.22781\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.34654\n",
      "Epoch [2/20],Step:[84/96], Loss: 1.33606\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.25441\n",
      "Epoch [3/20],Step:[12/96], Loss: 1.11769\n",
      "Epoch [3/20],Step:[24/96], Loss: 1.03279\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.35253\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.33952\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.29213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20],Step:[72/96], Loss: 1.14150\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.26484\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.24953\n",
      "Epoch [4/20],Step:[12/96], Loss: 1.17360\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.31541\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.96946\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.30726\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.26518\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.84834\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.86502\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.88012\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.26693\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.74239\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.30920\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.31587\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.16822\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.29021\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.67360\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.21321\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.40772\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.62973\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.13062\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.49065\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.42930\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.30452\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.50772\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.48953\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.21917\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.46459\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.40794\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.51159\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.32273\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.27601\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.11504\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.35450\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.26209\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.26086\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.21200\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.25334\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.29792\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.08753\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.19693\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.07195\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.14195\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.07406\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.21826\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.04899\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.09879\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.06703\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.20991\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.33317\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.06941\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.23733\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.19597\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.18005\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.04887\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.08572\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.04160\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.21045\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.10630\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.14400\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.15669\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.14918\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.05019\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.04066\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.02840\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.16110\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.06584\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.12912\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.14196\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.01646\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.04843\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.11391\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.01936\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.09643\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.00836\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.02542\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.02331\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.01777\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.01119\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.08363\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.01138\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.03204\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.05601\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.11233\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.00982\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.05263\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.03277\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.08112\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.04647\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.02321\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.11689\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.01008\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.04112\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.05520\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.04359\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.03395\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.04449\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.01281\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.06893\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.01386\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.00814\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.02031\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.00894\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.04007\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.00637\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.00834\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.00486\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.12865\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.06000\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.01060\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.00411\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.00398\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.03284\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.00858\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.00429\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.02462\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.00456\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.02036\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.00608\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.00348\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.04657\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.05694\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.00742\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.00684\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.01802\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.03257\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.00310\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.01233\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.11234\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00733\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.05301\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.02197\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.00289\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.00994\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.04101\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.01612\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.00519\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.00679\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 0.66308\n",
      "Epoch [1/20],Step:[24/96], Loss: 0.71770\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.75764\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.50084\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.85577\n",
      "Epoch [1/20],Step:[72/96], Loss: 0.33788\n",
      "Epoch [1/20],Step:[84/96], Loss: 0.85314\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.92709\n",
      "Epoch [2/20],Step:[12/96], Loss: 0.29269\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.81048\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.82137\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.82806\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.79317\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.31245\n",
      "Epoch [2/20],Step:[84/96], Loss: 0.15994\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.84532\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.22059\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.19455\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.19036\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.16608\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.17883\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.73070\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.66782\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.26740\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.61751\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.60848\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.14767\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.15882\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.63467\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.11285\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.19598\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.56362\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.24074\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.31299\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.56901\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.22452\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.16906\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.13073\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.12304\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.07215\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.07060\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.09465\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.05809\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.10495\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.34647\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.23646\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.04615\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.32547\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.29358\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.26420\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.06533\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.08967\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.30989\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.04974\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.24071\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.05112\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.08070\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.10992\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.19694\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.04594\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.16477\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.00713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20],Step:[84/96], Loss: 0.06237\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.29052\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.17503\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.03185\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.15836\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.02439\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.08634\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.01724\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.07037\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.03966\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.17619\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.19058\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.03005\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.12912\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.00770\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.00978\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.13761\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.12843\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.15792\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.12915\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.02628\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.09821\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.08842\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.21978\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.00545\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.06747\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.00455\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.16499\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.07292\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.00890\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.07080\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.01312\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.05792\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.11775\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.09384\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.00272\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.03408\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.02795\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.09535\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.03813\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.01150\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.02572\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.03463\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.07254\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.04303\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.04140\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.00159\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.10400\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.02902\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.00148\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.00294\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.03096\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.03771\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.00412\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.00149\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.00892\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.00167\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.06555\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.00271\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.06722\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.00105\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.02652\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.02607\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.03867\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.01787\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.01529\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.00231\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.00140\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.02063\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.02950\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.03539\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.00188\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.03858\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.02984\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.02911\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.00244\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.01473\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.01346\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.00046\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.05563\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.01744\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.01330\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.01219\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.00127\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.02476\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.02341\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.00040\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.01113\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.00017\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00723\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.02421\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.00037\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.00044\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.01935\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.02367\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.03330\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.02766\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.03960\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 0.24072\n",
      "Epoch [1/20],Step:[24/96], Loss: 1.32957\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.27607\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.35201\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.35422\n",
      "Epoch [1/20],Step:[72/96], Loss: 0.59648\n",
      "Epoch [1/20],Step:[84/96], Loss: 0.46063\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.42548\n",
      "Epoch [2/20],Step:[12/96], Loss: 0.54720\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.26660\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.24744\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.17019\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.16702\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.61581\n",
      "Epoch [2/20],Step:[84/96], Loss: 0.59221\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.63708\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.57276\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.16711\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.34224\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.56389\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.22311\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.11475\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.41402\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.07908\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.57805\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.09677\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.46745\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.18200\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.03077\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.17817\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.08844\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.33201\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.21214\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.46745\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.39372\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.19578\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.28761\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.26212\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.07441\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.05690\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.05999\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.02614\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.01906\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.03981\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.14949\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.28875\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.04540\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.17586\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.07988\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.02675\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.22330\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.24803\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.04128\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.20901\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.32208\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.19911\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.02082\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.11606\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.02369\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.00541\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.03510\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.03967\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.17160\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.16842\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.18692\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.01340\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.00758\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.12161\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.14304\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.06544\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.00106\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.01458\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.15546\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.05426\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.00464\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.00279\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.04790\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.01273\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.13924\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.00383\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.13293\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.19227\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.00189\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.10943\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.09642\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.00597\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.01020\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.07247\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.02100\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.00787\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.10135\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.00118\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.08260\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.08080\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.07773\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.01806\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.06818\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.00865\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.04035\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.00627\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.06189\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.00126\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.13174\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.00073\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.09580\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.05897\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.00098\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.07567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20],Step:[60/96], Loss: 0.01807\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.02759\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.09912\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.00342\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.07646\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.00062\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.04688\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.07557\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.03884\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.02201\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.00476\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.01263\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.00124\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.05294\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.00075\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.00108\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.04798\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.02666\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.00256\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.00399\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.05827\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.00061\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.03477\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.02907\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.04537\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.07262\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.07555\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.00029\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.04894\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.06087\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.02451\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.00030\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.03838\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.00025\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.02910\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.03871\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.02650\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.02152\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.05513\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.00060\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.01774\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.00074\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.00031\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00463\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.03071\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.00024\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.00134\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.04410\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.00304\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.01657\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.01239\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.00486\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 0.50214\n",
      "Epoch [1/20],Step:[24/96], Loss: 0.44622\n",
      "Epoch [1/20],Step:[36/96], Loss: 0.45262\n",
      "Epoch [1/20],Step:[48/96], Loss: 0.91799\n",
      "Epoch [1/20],Step:[60/96], Loss: 0.38926\n",
      "Epoch [1/20],Step:[72/96], Loss: 0.38677\n",
      "Epoch [1/20],Step:[84/96], Loss: 0.90937\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.88367\n",
      "Epoch [2/20],Step:[12/96], Loss: 0.89474\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.82930\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.80014\n",
      "Epoch [2/20],Step:[48/96], Loss: 0.27768\n",
      "Epoch [2/20],Step:[60/96], Loss: 0.22387\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.25405\n",
      "Epoch [2/20],Step:[84/96], Loss: 0.26621\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.77431\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.55916\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.56394\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.50700\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.21237\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.24255\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.47971\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.57307\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.31707\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.30919\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.30529\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.57178\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.11713\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.48808\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.26368\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.24554\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.19472\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.37399\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.31116\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.29975\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.10232\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.13769\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.29018\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.24091\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.29857\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.33742\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.30943\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.23771\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.23424\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.24624\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.23016\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.07976\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.15043\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.04916\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.04133\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.22731\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.21436\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.18763\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.12807\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.10248\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.02265\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.10613\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.23183\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.13211\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.11273\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.14768\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.19103\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.15312\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.03980\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.01583\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.12422\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.09632\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.02216\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.04168\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.14562\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.01499\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.07295\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.06787\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.01265\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.13880\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.05507\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.01411\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.01256\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.02221\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.06279\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.07647\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.02241\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.00675\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.08575\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.01096\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.10519\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.09261\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.00730\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.00579\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.00998\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.03665\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.00989\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.07541\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.08736\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.01929\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.00668\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.00477\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.03493\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.05762\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.06093\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.00367\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.00424\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.01998\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.02023\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.00994\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.03144\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.01978\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.05513\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.01917\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.01638\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.01124\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.00787\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.06162\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.06079\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.08350\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.00957\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.02204\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.01008\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.04601\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.00499\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.01425\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.00119\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.00283\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.03443\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.02654\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.04620\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.00239\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.00239\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.08627\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.02758\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.01401\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.00671\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.00841\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.00159\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.02304\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.02586\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.01048\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.00452\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.04263\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.00543\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.02047\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.02358\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.00130\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.00160\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.01386\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.00589\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.02372\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.02085\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.00179\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.00373\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.00395\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.00695\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.00308\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.00388\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.00176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20],Step:[48/96], Loss: 0.03102\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.00540\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.00285\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.00289\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.00967\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch [1/20],Step:[12/96], Loss: 2.57350\n",
      "Epoch [1/20],Step:[24/96], Loss: 0.15267\n",
      "Epoch [1/20],Step:[36/96], Loss: 2.30316\n",
      "Epoch [1/20],Step:[48/96], Loss: 1.74855\n",
      "Epoch [1/20],Step:[60/96], Loss: 1.60081\n",
      "Epoch [1/20],Step:[72/96], Loss: 1.51969\n",
      "Epoch [1/20],Step:[84/96], Loss: 1.51318\n",
      "Epoch [1/20],Step:[96/96], Loss: 0.34119\n",
      "Epoch [2/20],Step:[12/96], Loss: 0.35571\n",
      "Epoch [2/20],Step:[24/96], Loss: 0.37920\n",
      "Epoch [2/20],Step:[36/96], Loss: 0.39323\n",
      "Epoch [2/20],Step:[48/96], Loss: 1.13630\n",
      "Epoch [2/20],Step:[60/96], Loss: 1.09031\n",
      "Epoch [2/20],Step:[72/96], Loss: 0.98586\n",
      "Epoch [2/20],Step:[84/96], Loss: 0.94523\n",
      "Epoch [2/20],Step:[96/96], Loss: 0.89029\n",
      "Epoch [3/20],Step:[12/96], Loss: 0.81279\n",
      "Epoch [3/20],Step:[24/96], Loss: 0.76892\n",
      "Epoch [3/20],Step:[36/96], Loss: 0.69420\n",
      "Epoch [3/20],Step:[48/96], Loss: 0.68886\n",
      "Epoch [3/20],Step:[60/96], Loss: 0.69317\n",
      "Epoch [3/20],Step:[72/96], Loss: 0.66493\n",
      "Epoch [3/20],Step:[84/96], Loss: 0.67282\n",
      "Epoch [3/20],Step:[96/96], Loss: 0.60184\n",
      "Epoch [4/20],Step:[12/96], Loss: 0.72856\n",
      "Epoch [4/20],Step:[24/96], Loss: 0.55866\n",
      "Epoch [4/20],Step:[36/96], Loss: 0.76205\n",
      "Epoch [4/20],Step:[48/96], Loss: 0.52032\n",
      "Epoch [4/20],Step:[60/96], Loss: 0.47687\n",
      "Epoch [4/20],Step:[72/96], Loss: 0.81713\n",
      "Epoch [4/20],Step:[84/96], Loss: 0.45524\n",
      "Epoch [4/20],Step:[96/96], Loss: 0.81789\n",
      "Epoch [5/20],Step:[12/96], Loss: 0.50731\n",
      "Epoch [5/20],Step:[24/96], Loss: 0.34337\n",
      "Epoch [5/20],Step:[36/96], Loss: 0.34632\n",
      "Epoch [5/20],Step:[48/96], Loss: 0.38939\n",
      "Epoch [5/20],Step:[60/96], Loss: 0.40541\n",
      "Epoch [5/20],Step:[72/96], Loss: 0.35771\n",
      "Epoch [5/20],Step:[84/96], Loss: 0.31875\n",
      "Epoch [5/20],Step:[96/96], Loss: 0.88918\n",
      "Epoch [6/20],Step:[12/96], Loss: 0.91166\n",
      "Epoch [6/20],Step:[24/96], Loss: 0.33190\n",
      "Epoch [6/20],Step:[36/96], Loss: 0.88563\n",
      "Epoch [6/20],Step:[48/96], Loss: 0.39515\n",
      "Epoch [6/20],Step:[60/96], Loss: 0.33889\n",
      "Epoch [6/20],Step:[72/96], Loss: 0.39491\n",
      "Epoch [6/20],Step:[84/96], Loss: 0.96690\n",
      "Epoch [6/20],Step:[96/96], Loss: 0.34738\n",
      "Epoch [7/20],Step:[12/96], Loss: 0.29611\n",
      "Epoch [7/20],Step:[24/96], Loss: 0.95978\n",
      "Epoch [7/20],Step:[36/96], Loss: 0.32456\n",
      "Epoch [7/20],Step:[48/96], Loss: 0.92497\n",
      "Epoch [7/20],Step:[60/96], Loss: 0.33011\n",
      "Epoch [7/20],Step:[72/96], Loss: 0.94805\n",
      "Epoch [7/20],Step:[84/96], Loss: 0.20440\n",
      "Epoch [7/20],Step:[96/96], Loss: 0.27063\n",
      "Epoch [8/20],Step:[12/96], Loss: 0.93134\n",
      "Epoch [8/20],Step:[24/96], Loss: 0.33243\n",
      "Epoch [8/20],Step:[36/96], Loss: 0.23396\n",
      "Epoch [8/20],Step:[48/96], Loss: 0.35685\n",
      "Epoch [8/20],Step:[60/96], Loss: 0.85693\n",
      "Epoch [8/20],Step:[72/96], Loss: 0.38141\n",
      "Epoch [8/20],Step:[84/96], Loss: 0.87594\n",
      "Epoch [8/20],Step:[96/96], Loss: 0.27739\n",
      "Epoch [9/20],Step:[12/96], Loss: 0.92411\n",
      "Epoch [9/20],Step:[24/96], Loss: 0.31216\n",
      "Epoch [9/20],Step:[36/96], Loss: 0.30170\n",
      "Epoch [9/20],Step:[48/96], Loss: 0.25258\n",
      "Epoch [9/20],Step:[60/96], Loss: 0.84526\n",
      "Epoch [9/20],Step:[72/96], Loss: 0.24939\n",
      "Epoch [9/20],Step:[84/96], Loss: 0.36111\n",
      "Epoch [9/20],Step:[96/96], Loss: 0.26407\n",
      "Epoch [10/20],Step:[12/96], Loss: 0.25674\n",
      "Epoch [10/20],Step:[24/96], Loss: 0.21910\n",
      "Epoch [10/20],Step:[36/96], Loss: 0.22050\n",
      "Epoch [10/20],Step:[48/96], Loss: 0.80211\n",
      "Epoch [10/20],Step:[60/96], Loss: 0.21063\n",
      "Epoch [10/20],Step:[72/96], Loss: 0.88402\n",
      "Epoch [10/20],Step:[84/96], Loss: 0.22702\n",
      "Epoch [10/20],Step:[96/96], Loss: 0.24135\n",
      "Epoch [11/20],Step:[12/96], Loss: 0.22178\n",
      "Epoch [11/20],Step:[24/96], Loss: 0.18492\n",
      "Epoch [11/20],Step:[36/96], Loss: 0.84187\n",
      "Epoch [11/20],Step:[48/96], Loss: 0.26171\n",
      "Epoch [11/20],Step:[60/96], Loss: 0.81032\n",
      "Epoch [11/20],Step:[72/96], Loss: 0.76322\n",
      "Epoch [11/20],Step:[84/96], Loss: 0.76988\n",
      "Epoch [11/20],Step:[96/96], Loss: 0.14985\n",
      "Epoch [12/20],Step:[12/96], Loss: 0.12281\n",
      "Epoch [12/20],Step:[24/96], Loss: 0.26252\n",
      "Epoch [12/20],Step:[36/96], Loss: 0.26972\n",
      "Epoch [12/20],Step:[48/96], Loss: 0.25962\n",
      "Epoch [12/20],Step:[60/96], Loss: 0.61975\n",
      "Epoch [12/20],Step:[72/96], Loss: 0.26132\n",
      "Epoch [12/20],Step:[84/96], Loss: 0.61407\n",
      "Epoch [12/20],Step:[96/96], Loss: 0.24746\n",
      "Epoch [13/20],Step:[12/96], Loss: 0.63118\n",
      "Epoch [13/20],Step:[24/96], Loss: 0.15881\n",
      "Epoch [13/20],Step:[36/96], Loss: 0.64606\n",
      "Epoch [13/20],Step:[48/96], Loss: 0.58368\n",
      "Epoch [13/20],Step:[60/96], Loss: 0.23921\n",
      "Epoch [13/20],Step:[72/96], Loss: 0.13305\n",
      "Epoch [13/20],Step:[84/96], Loss: 0.58400\n",
      "Epoch [13/20],Step:[96/96], Loss: 0.12846\n",
      "Epoch [14/20],Step:[12/96], Loss: 0.12427\n",
      "Epoch [14/20],Step:[24/96], Loss: 0.10540\n",
      "Epoch [14/20],Step:[36/96], Loss: 0.62107\n",
      "Epoch [14/20],Step:[48/96], Loss: 0.12195\n",
      "Epoch [14/20],Step:[60/96], Loss: 0.24362\n",
      "Epoch [14/20],Step:[72/96], Loss: 0.26990\n",
      "Epoch [14/20],Step:[84/96], Loss: 0.11954\n",
      "Epoch [14/20],Step:[96/96], Loss: 0.50870\n",
      "Epoch [15/20],Step:[12/96], Loss: 0.45398\n",
      "Epoch [15/20],Step:[24/96], Loss: 0.13381\n",
      "Epoch [15/20],Step:[36/96], Loss: 0.50595\n",
      "Epoch [15/20],Step:[48/96], Loss: 0.41751\n",
      "Epoch [15/20],Step:[60/96], Loss: 0.10675\n",
      "Epoch [15/20],Step:[72/96], Loss: 0.40870\n",
      "Epoch [15/20],Step:[84/96], Loss: 0.06632\n",
      "Epoch [15/20],Step:[96/96], Loss: 0.19176\n",
      "Epoch [16/20],Step:[12/96], Loss: 0.48213\n",
      "Epoch [16/20],Step:[24/96], Loss: 0.12333\n",
      "Epoch [16/20],Step:[36/96], Loss: 0.07291\n",
      "Epoch [16/20],Step:[48/96], Loss: 0.19107\n",
      "Epoch [16/20],Step:[60/96], Loss: 0.06866\n",
      "Epoch [16/20],Step:[72/96], Loss: 0.12830\n",
      "Epoch [16/20],Step:[84/96], Loss: 0.34375\n",
      "Epoch [16/20],Step:[96/96], Loss: 0.32500\n",
      "Epoch [17/20],Step:[12/96], Loss: 0.41024\n",
      "Epoch [17/20],Step:[24/96], Loss: 0.33261\n",
      "Epoch [17/20],Step:[36/96], Loss: 0.30080\n",
      "Epoch [17/20],Step:[48/96], Loss: 0.30878\n",
      "Epoch [17/20],Step:[60/96], Loss: 0.14816\n",
      "Epoch [17/20],Step:[72/96], Loss: 0.38178\n",
      "Epoch [17/20],Step:[84/96], Loss: 0.04159\n",
      "Epoch [17/20],Step:[96/96], Loss: 0.17753\n",
      "Epoch [18/20],Step:[12/96], Loss: 0.05896\n",
      "Epoch [18/20],Step:[24/96], Loss: 0.14502\n",
      "Epoch [18/20],Step:[36/96], Loss: 0.37620\n",
      "Epoch [18/20],Step:[48/96], Loss: 0.33965\n",
      "Epoch [18/20],Step:[60/96], Loss: 0.02776\n",
      "Epoch [18/20],Step:[72/96], Loss: 0.33893\n",
      "Epoch [18/20],Step:[84/96], Loss: 0.12208\n",
      "Epoch [18/20],Step:[96/96], Loss: 0.31704\n",
      "Epoch [19/20],Step:[12/96], Loss: 0.11730\n",
      "Epoch [19/20],Step:[24/96], Loss: 0.17328\n",
      "Epoch [19/20],Step:[36/96], Loss: 0.15512\n",
      "Epoch [19/20],Step:[48/96], Loss: 0.18529\n",
      "Epoch [19/20],Step:[60/96], Loss: 0.14601\n",
      "Epoch [19/20],Step:[72/96], Loss: 0.01549\n",
      "Epoch [19/20],Step:[84/96], Loss: 0.28330\n",
      "Epoch [19/20],Step:[96/96], Loss: 0.03306\n",
      "Epoch [20/20],Step:[12/96], Loss: 0.19367\n",
      "Epoch [20/20],Step:[24/96], Loss: 0.02668\n",
      "Epoch [20/20],Step:[36/96], Loss: 0.10985\n",
      "Epoch [20/20],Step:[48/96], Loss: 0.03401\n",
      "Epoch [20/20],Step:[60/96], Loss: 0.21433\n",
      "Epoch [20/20],Step:[72/96], Loss: 0.06437\n",
      "Epoch [20/20],Step:[84/96], Loss: 0.25979\n",
      "Epoch [20/20],Step:[96/96], Loss: 0.02106\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "{1: 100.0, 2: 100.0, 3: 100.0, 4: 100.0, 5: 100.0, 6: 100.0, 7: 100.0, 8: 100.0, 9: 100.0, 10: 100.0}\n"
     ]
    }
   ],
   "source": [
    "n_hidden=(i for i in range(1,11))\n",
    "storage=dict()\n",
    "\n",
    "for i in n_hidden:\n",
    "    net=One_hidden(i)\n",
    "    storage[i]=train_with_steps(net,trainloader)\n",
    "    \n",
    "print(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20],Step:[12/120], Loss: 2.87430\n",
      "Epoch [1/20],Step:[24/120], Loss: 0.03145\n",
      "Epoch [1/20],Step:[36/120], Loss: 2.47203\n",
      "Epoch [1/20],Step:[48/120], Loss: 0.03390\n",
      "Epoch [1/20],Step:[60/120], Loss: 0.08064\n",
      "Epoch [1/20],Step:[72/120], Loss: 0.06139\n",
      "Epoch [1/20],Step:[84/120], Loss: 2.27924\n",
      "Epoch [1/20],Step:[96/120], Loss: 0.04288\n",
      "Epoch [1/20],Step:[108/120], Loss: 2.36179\n",
      "Epoch [1/20],Step:[120/120], Loss: 2.23935\n",
      "Epoch [2/20],Step:[12/120], Loss: 0.19593\n",
      "Epoch [2/20],Step:[24/120], Loss: 0.14459\n",
      "Epoch [2/20],Step:[36/120], Loss: 1.66994\n",
      "Epoch [2/20],Step:[48/120], Loss: 0.09422\n",
      "Epoch [2/20],Step:[60/120], Loss: 1.56626\n",
      "Epoch [2/20],Step:[72/120], Loss: 0.15079\n",
      "Epoch [2/20],Step:[84/120], Loss: 1.35380\n",
      "Epoch [2/20],Step:[96/120], Loss: 0.13182\n",
      "Epoch [2/20],Step:[108/120], Loss: 0.17285\n",
      "Epoch [2/20],Step:[120/120], Loss: 0.17623\n",
      "Epoch [3/20],Step:[12/120], Loss: 0.18032\n",
      "Epoch [3/20],Step:[24/120], Loss: 0.16995\n",
      "Epoch [3/20],Step:[36/120], Loss: 1.09198\n",
      "Epoch [3/20],Step:[48/120], Loss: 0.18724\n",
      "Epoch [3/20],Step:[60/120], Loss: 0.24662\n",
      "Epoch [3/20],Step:[72/120], Loss: 0.21659\n",
      "Epoch [3/20],Step:[84/120], Loss: 0.24103\n",
      "Epoch [3/20],Step:[96/120], Loss: 0.10554\n",
      "Epoch [3/20],Step:[108/120], Loss: 0.82779\n",
      "Epoch [3/20],Step:[120/120], Loss: 0.10893\n",
      "Epoch [4/20],Step:[12/120], Loss: 0.18086\n",
      "Epoch [4/20],Step:[24/120], Loss: 0.10759\n",
      "Epoch [4/20],Step:[36/120], Loss: 0.09753\n",
      "Epoch [4/20],Step:[48/120], Loss: 0.09867\n",
      "Epoch [4/20],Step:[60/120], Loss: 0.11337\n",
      "Epoch [4/20],Step:[72/120], Loss: 0.71651\n",
      "Epoch [4/20],Step:[84/120], Loss: 0.12068\n",
      "Epoch [4/20],Step:[96/120], Loss: 0.67687\n",
      "Epoch [4/20],Step:[108/120], Loss: 0.63529\n",
      "Epoch [4/20],Step:[120/120], Loss: 0.12772\n",
      "Epoch [5/20],Step:[12/120], Loss: 0.06491\n",
      "Epoch [5/20],Step:[24/120], Loss: 0.21781\n",
      "Epoch [5/20],Step:[36/120], Loss: 0.04638\n",
      "Epoch [5/20],Step:[48/120], Loss: 0.55727\n",
      "Epoch [5/20],Step:[60/120], Loss: 0.11592\n",
      "Epoch [5/20],Step:[72/120], Loss: 0.26615\n",
      "Epoch [5/20],Step:[84/120], Loss: 0.07682\n",
      "Epoch [5/20],Step:[96/120], Loss: 0.04713\n",
      "Epoch [5/20],Step:[108/120], Loss: 0.45914\n",
      "Epoch [5/20],Step:[120/120], Loss: 0.05842\n",
      "Epoch [6/20],Step:[12/120], Loss: 0.39901\n",
      "Epoch [6/20],Step:[24/120], Loss: 0.13155\n",
      "Epoch [6/20],Step:[36/120], Loss: 0.18962\n",
      "Epoch [6/20],Step:[48/120], Loss: 0.45624\n",
      "Epoch [6/20],Step:[60/120], Loss: 0.11731\n",
      "Epoch [6/20],Step:[72/120], Loss: 0.39116\n",
      "Epoch [6/20],Step:[84/120], Loss: 0.11224\n",
      "Epoch [6/20],Step:[96/120], Loss: 0.32446\n",
      "Epoch [6/20],Step:[108/120], Loss: 0.06757\n",
      "Epoch [6/20],Step:[120/120], Loss: 0.06077\n",
      "Epoch [7/20],Step:[12/120], Loss: 0.01601\n",
      "Epoch [7/20],Step:[24/120], Loss: 0.04059\n",
      "Epoch [7/20],Step:[36/120], Loss: 0.08394\n",
      "Epoch [7/20],Step:[48/120], Loss: 0.09513\n",
      "Epoch [7/20],Step:[60/120], Loss: 0.06686\n",
      "Epoch [7/20],Step:[72/120], Loss: 0.23757\n",
      "Epoch [7/20],Step:[84/120], Loss: 0.02258\n",
      "Epoch [7/20],Step:[96/120], Loss: 0.22461\n",
      "Epoch [7/20],Step:[108/120], Loss: 0.28807\n",
      "Epoch [7/20],Step:[120/120], Loss: 0.22153\n",
      "Epoch [8/20],Step:[12/120], Loss: 0.19547\n",
      "Epoch [8/20],Step:[24/120], Loss: 0.01396\n",
      "Epoch [8/20],Step:[36/120], Loss: 0.11002\n",
      "Epoch [8/20],Step:[48/120], Loss: 0.24791\n",
      "Epoch [8/20],Step:[60/120], Loss: 0.19635\n",
      "Epoch [8/20],Step:[72/120], Loss: 0.02598\n",
      "Epoch [8/20],Step:[84/120], Loss: 0.15815\n",
      "Epoch [8/20],Step:[96/120], Loss: 0.01468\n",
      "Epoch [8/20],Step:[108/120], Loss: 0.17277\n",
      "Epoch [8/20],Step:[120/120], Loss: 0.19289\n",
      "Epoch [9/20],Step:[12/120], Loss: 0.01278\n",
      "Epoch [9/20],Step:[24/120], Loss: 0.08429\n",
      "Epoch [9/20],Step:[36/120], Loss: 0.01362\n",
      "Epoch [9/20],Step:[48/120], Loss: 0.10830\n",
      "Epoch [9/20],Step:[60/120], Loss: 0.00492\n",
      "Epoch [9/20],Step:[72/120], Loss: 0.12542\n",
      "Epoch [9/20],Step:[84/120], Loss: 0.13467\n",
      "Epoch [9/20],Step:[96/120], Loss: 0.04669\n",
      "Epoch [9/20],Step:[108/120], Loss: 0.00791\n",
      "Epoch [9/20],Step:[120/120], Loss: 0.00361\n",
      "Epoch [10/20],Step:[12/120], Loss: 0.12298\n",
      "Epoch [10/20],Step:[24/120], Loss: 0.12324\n",
      "Epoch [10/20],Step:[36/120], Loss: 0.08026\n",
      "Epoch [10/20],Step:[48/120], Loss: 0.10789\n",
      "Epoch [10/20],Step:[60/120], Loss: 0.11028\n",
      "Epoch [10/20],Step:[72/120], Loss: 0.00536\n",
      "Epoch [10/20],Step:[84/120], Loss: 0.05686\n",
      "Epoch [10/20],Step:[96/120], Loss: 0.12041\n",
      "Epoch [10/20],Step:[108/120], Loss: 0.01389\n",
      "Epoch [10/20],Step:[120/120], Loss: 0.00351\n",
      "Epoch [11/20],Step:[12/120], Loss: 0.11541\n",
      "Epoch [11/20],Step:[24/120], Loss: 0.01051\n",
      "Epoch [11/20],Step:[36/120], Loss: 0.07153\n",
      "Epoch [11/20],Step:[48/120], Loss: 0.02201\n",
      "Epoch [11/20],Step:[60/120], Loss: 0.09307\n",
      "Epoch [11/20],Step:[72/120], Loss: 0.00484\n",
      "Epoch [11/20],Step:[84/120], Loss: 0.00572\n",
      "Epoch [11/20],Step:[96/120], Loss: 0.08106\n",
      "Epoch [11/20],Step:[108/120], Loss: 0.08366\n",
      "Epoch [11/20],Step:[120/120], Loss: 0.08773\n",
      "Epoch [12/20],Step:[12/120], Loss: 0.00641\n",
      "Epoch [12/20],Step:[24/120], Loss: 0.00831\n",
      "Epoch [12/20],Step:[36/120], Loss: 0.00113\n",
      "Epoch [12/20],Step:[48/120], Loss: 0.12836\n",
      "Epoch [12/20],Step:[60/120], Loss: 0.00287\n",
      "Epoch [12/20],Step:[72/120], Loss: 0.00753\n",
      "Epoch [12/20],Step:[84/120], Loss: 0.00094\n",
      "Epoch [12/20],Step:[96/120], Loss: 0.00694\n",
      "Epoch [12/20],Step:[108/120], Loss: 0.13807\n",
      "Epoch [12/20],Step:[120/120], Loss: 0.06026\n",
      "Epoch [13/20],Step:[12/120], Loss: 0.00240\n",
      "Epoch [13/20],Step:[24/120], Loss: 0.13306\n",
      "Epoch [13/20],Step:[36/120], Loss: 0.02335\n",
      "Epoch [13/20],Step:[48/120], Loss: 0.04019\n",
      "Epoch [13/20],Step:[60/120], Loss: 0.02296\n",
      "Epoch [13/20],Step:[72/120], Loss: 0.00011\n",
      "Epoch [13/20],Step:[84/120], Loss: 0.02253\n",
      "Epoch [13/20],Step:[96/120], Loss: 0.00071\n",
      "Epoch [13/20],Step:[108/120], Loss: 0.01895\n",
      "Epoch [13/20],Step:[120/120], Loss: 0.00023\n",
      "Epoch [14/20],Step:[12/120], Loss: 0.02531\n",
      "Epoch [14/20],Step:[24/120], Loss: 0.04523\n",
      "Epoch [14/20],Step:[36/120], Loss: 0.00251\n",
      "Epoch [14/20],Step:[48/120], Loss: 0.05231\n",
      "Epoch [14/20],Step:[60/120], Loss: 0.09412\n",
      "Epoch [14/20],Step:[72/120], Loss: 0.00083\n",
      "Epoch [14/20],Step:[84/120], Loss: 0.00017\n",
      "Epoch [14/20],Step:[96/120], Loss: 0.03461\n",
      "Epoch [14/20],Step:[108/120], Loss: 0.00295\n",
      "Epoch [14/20],Step:[120/120], Loss: 0.00079\n",
      "Epoch [15/20],Step:[12/120], Loss: 0.04507\n",
      "Epoch [15/20],Step:[24/120], Loss: 0.00990\n",
      "Epoch [15/20],Step:[36/120], Loss: 0.00093\n",
      "Epoch [15/20],Step:[48/120], Loss: 0.02317\n",
      "Epoch [15/20],Step:[60/120], Loss: 0.04306\n",
      "Epoch [15/20],Step:[72/120], Loss: 0.00064\n",
      "Epoch [15/20],Step:[84/120], Loss: 0.01873\n",
      "Epoch [15/20],Step:[96/120], Loss: 0.00034\n",
      "Epoch [15/20],Step:[108/120], Loss: 0.04341\n",
      "Epoch [15/20],Step:[120/120], Loss: 0.00947\n",
      "Epoch [16/20],Step:[12/120], Loss: 0.00114\n",
      "Epoch [16/20],Step:[24/120], Loss: 0.00026\n",
      "Epoch [16/20],Step:[36/120], Loss: 0.00047\n",
      "Epoch [16/20],Step:[48/120], Loss: 0.02543\n",
      "Epoch [16/20],Step:[60/120], Loss: 0.00071\n",
      "Epoch [16/20],Step:[72/120], Loss: 0.00189\n",
      "Epoch [16/20],Step:[84/120], Loss: 0.00211\n",
      "Epoch [16/20],Step:[96/120], Loss: 0.01386\n",
      "Epoch [16/20],Step:[108/120], Loss: 0.00014\n",
      "Epoch [16/20],Step:[120/120], Loss: 0.01676\n",
      "Epoch [17/20],Step:[12/120], Loss: 0.00013\n",
      "Epoch [17/20],Step:[24/120], Loss: 0.02483\n",
      "Epoch [17/20],Step:[36/120], Loss: 0.02832\n",
      "Epoch [17/20],Step:[48/120], Loss: 0.02690\n",
      "Epoch [17/20],Step:[60/120], Loss: 0.02518\n",
      "Epoch [17/20],Step:[72/120], Loss: 0.01976\n",
      "Epoch [17/20],Step:[84/120], Loss: 0.02862\n",
      "Epoch [17/20],Step:[96/120], Loss: 0.03817\n",
      "Epoch [17/20],Step:[108/120], Loss: 0.00019\n",
      "Epoch [17/20],Step:[120/120], Loss: 0.01051\n",
      "Epoch [18/20],Step:[12/120], Loss: 0.00007\n",
      "Epoch [18/20],Step:[24/120], Loss: 0.00050\n",
      "Epoch [18/20],Step:[36/120], Loss: 0.01214\n",
      "Epoch [18/20],Step:[48/120], Loss: 0.01828\n",
      "Epoch [18/20],Step:[60/120], Loss: 0.00177\n",
      "Epoch [18/20],Step:[72/120], Loss: 0.00451\n",
      "Epoch [18/20],Step:[84/120], Loss: 0.00051\n",
      "Epoch [18/20],Step:[96/120], Loss: 0.00076\n",
      "Epoch [18/20],Step:[108/120], Loss: 0.02107\n",
      "Epoch [18/20],Step:[120/120], Loss: 0.06389\n",
      "Epoch [19/20],Step:[12/120], Loss: 0.03041\n",
      "Epoch [19/20],Step:[24/120], Loss: 0.00005\n",
      "Epoch [19/20],Step:[36/120], Loss: 0.00860\n",
      "Epoch [19/20],Step:[48/120], Loss: 0.00081\n",
      "Epoch [19/20],Step:[60/120], Loss: 0.03611\n",
      "Epoch [19/20],Step:[72/120], Loss: 0.06640\n",
      "Epoch [19/20],Step:[84/120], Loss: 0.00443\n",
      "Epoch [19/20],Step:[96/120], Loss: 0.00028\n",
      "Epoch [19/20],Step:[108/120], Loss: 0.01542\n",
      "Epoch [19/20],Step:[120/120], Loss: 0.00755\n",
      "Epoch [20/20],Step:[12/120], Loss: 0.02368\n",
      "Epoch [20/20],Step:[24/120], Loss: 0.00029\n",
      "Epoch [20/20],Step:[36/120], Loss: 0.00575\n",
      "Epoch [20/20],Step:[48/120], Loss: 0.00009\n",
      "Epoch [20/20],Step:[60/120], Loss: 0.00004\n",
      "Epoch [20/20],Step:[72/120], Loss: 0.01323\n",
      "Epoch [20/20],Step:[84/120], Loss: 0.00028\n",
      "Epoch [20/20],Step:[96/120], Loss: 0.00027\n",
      "Epoch [20/20],Step:[108/120], Loss: 0.00557\n",
      "Epoch [20/20],Step:[120/120], Loss: 0.00201\n",
      "Accuracy of the network for val data: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_best=One_hidden(10)\n",
    "train_with_steps(net_best,mainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network for test data: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_test(net_best,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model through average loss per epoch:\n",
    "\n",
    "def model_train(net,trainloader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 30  # suggest training between 20-50 epochs\n",
    "    loss_values=[]\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "        train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "        for data, target in trainloader:\n",
    "        # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = net(data)\n",
    "        # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "        # update running training loss\n",
    "            train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "        train_loss = train_loss/len(trainloader.dataset)\n",
    "        loss_values.append(train_loss)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch+1, \n",
    "            train_loss\n",
    "            ))\n",
    "    plt.plot(loss_values)\n",
    "    acc=accuracy(net,valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.655440\n",
      "Epoch: 2 \tTraining Loss: 0.644630\n",
      "Epoch: 3 \tTraining Loss: 0.635170\n",
      "Epoch: 4 \tTraining Loss: 0.626182\n",
      "Epoch: 5 \tTraining Loss: 0.618577\n",
      "Epoch: 6 \tTraining Loss: 0.608312\n",
      "Epoch: 7 \tTraining Loss: 0.596889\n",
      "Epoch: 8 \tTraining Loss: 0.586028\n",
      "Epoch: 9 \tTraining Loss: 0.572818\n",
      "Epoch: 10 \tTraining Loss: 0.558471\n",
      "Epoch: 11 \tTraining Loss: 0.542316\n",
      "Epoch: 12 \tTraining Loss: 0.527281\n",
      "Epoch: 13 \tTraining Loss: 0.505899\n",
      "Epoch: 14 \tTraining Loss: 0.485529\n",
      "Epoch: 15 \tTraining Loss: 0.460068\n",
      "Epoch: 16 \tTraining Loss: 0.433625\n",
      "Epoch: 17 \tTraining Loss: 0.404346\n",
      "Epoch: 18 \tTraining Loss: 0.372580\n",
      "Epoch: 19 \tTraining Loss: 0.339231\n",
      "Epoch: 20 \tTraining Loss: 0.306803\n",
      "Epoch: 21 \tTraining Loss: 0.276137\n",
      "Epoch: 22 \tTraining Loss: 0.252708\n",
      "Epoch: 23 \tTraining Loss: 0.234159\n",
      "Epoch: 24 \tTraining Loss: 0.218801\n",
      "Epoch: 25 \tTraining Loss: 0.204591\n",
      "Epoch: 26 \tTraining Loss: 0.192137\n",
      "Epoch: 27 \tTraining Loss: 0.181125\n",
      "Epoch: 28 \tTraining Loss: 0.170658\n",
      "Epoch: 29 \tTraining Loss: 0.161168\n",
      "Epoch: 30 \tTraining Loss: 0.152011\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 1.051608\n",
      "Epoch: 2 \tTraining Loss: 0.826513\n",
      "Epoch: 3 \tTraining Loss: 0.700142\n",
      "Epoch: 4 \tTraining Loss: 0.634439\n",
      "Epoch: 5 \tTraining Loss: 0.599606\n",
      "Epoch: 6 \tTraining Loss: 0.577408\n",
      "Epoch: 7 \tTraining Loss: 0.561623\n",
      "Epoch: 8 \tTraining Loss: 0.546937\n",
      "Epoch: 9 \tTraining Loss: 0.530832\n",
      "Epoch: 10 \tTraining Loss: 0.514390\n",
      "Epoch: 11 \tTraining Loss: 0.496695\n",
      "Epoch: 12 \tTraining Loss: 0.476945\n",
      "Epoch: 13 \tTraining Loss: 0.455666\n",
      "Epoch: 14 \tTraining Loss: 0.431974\n",
      "Epoch: 15 \tTraining Loss: 0.406556\n",
      "Epoch: 16 \tTraining Loss: 0.379002\n",
      "Epoch: 17 \tTraining Loss: 0.349006\n",
      "Epoch: 18 \tTraining Loss: 0.317486\n",
      "Epoch: 19 \tTraining Loss: 0.287906\n",
      "Epoch: 20 \tTraining Loss: 0.262173\n",
      "Epoch: 21 \tTraining Loss: 0.241354\n",
      "Epoch: 22 \tTraining Loss: 0.224444\n",
      "Epoch: 23 \tTraining Loss: 0.210052\n",
      "Epoch: 24 \tTraining Loss: 0.197108\n",
      "Epoch: 25 \tTraining Loss: 0.185213\n",
      "Epoch: 26 \tTraining Loss: 0.174838\n",
      "Epoch: 27 \tTraining Loss: 0.165101\n",
      "Epoch: 28 \tTraining Loss: 0.156028\n",
      "Epoch: 29 \tTraining Loss: 0.147538\n",
      "Epoch: 30 \tTraining Loss: 0.139709\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 0.495798\n",
      "Epoch: 2 \tTraining Loss: 0.424577\n",
      "Epoch: 3 \tTraining Loss: 0.354489\n",
      "Epoch: 4 \tTraining Loss: 0.289205\n",
      "Epoch: 5 \tTraining Loss: 0.225080\n",
      "Epoch: 6 \tTraining Loss: 0.175106\n",
      "Epoch: 7 \tTraining Loss: 0.136172\n",
      "Epoch: 8 \tTraining Loss: 0.106817\n",
      "Epoch: 9 \tTraining Loss: 0.086032\n",
      "Epoch: 10 \tTraining Loss: 0.069784\n",
      "Epoch: 11 \tTraining Loss: 0.057396\n",
      "Epoch: 12 \tTraining Loss: 0.047406\n",
      "Epoch: 13 \tTraining Loss: 0.039737\n",
      "Epoch: 14 \tTraining Loss: 0.034001\n",
      "Epoch: 15 \tTraining Loss: 0.029181\n",
      "Epoch: 16 \tTraining Loss: 0.025342\n",
      "Epoch: 17 \tTraining Loss: 0.022023\n",
      "Epoch: 18 \tTraining Loss: 0.019448\n",
      "Epoch: 19 \tTraining Loss: 0.017058\n",
      "Epoch: 20 \tTraining Loss: 0.015258\n",
      "Epoch: 21 \tTraining Loss: 0.013512\n",
      "Epoch: 22 \tTraining Loss: 0.012085\n",
      "Epoch: 23 \tTraining Loss: 0.010849\n",
      "Epoch: 24 \tTraining Loss: 0.009828\n",
      "Epoch: 25 \tTraining Loss: 0.008976\n",
      "Epoch: 26 \tTraining Loss: 0.008063\n",
      "Epoch: 27 \tTraining Loss: 0.007366\n",
      "Epoch: 28 \tTraining Loss: 0.006759\n",
      "Epoch: 29 \tTraining Loss: 0.006172\n",
      "Epoch: 30 \tTraining Loss: 0.005642\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 1.039032\n",
      "Epoch: 2 \tTraining Loss: 0.756252\n",
      "Epoch: 3 \tTraining Loss: 0.656880\n",
      "Epoch: 4 \tTraining Loss: 0.620751\n",
      "Epoch: 5 \tTraining Loss: 0.598352\n",
      "Epoch: 6 \tTraining Loss: 0.582002\n",
      "Epoch: 7 \tTraining Loss: 0.565300\n",
      "Epoch: 8 \tTraining Loss: 0.551608\n",
      "Epoch: 9 \tTraining Loss: 0.535046\n",
      "Epoch: 10 \tTraining Loss: 0.516798\n",
      "Epoch: 11 \tTraining Loss: 0.498669\n",
      "Epoch: 12 \tTraining Loss: 0.473711\n",
      "Epoch: 13 \tTraining Loss: 0.441148\n",
      "Epoch: 14 \tTraining Loss: 0.403151\n",
      "Epoch: 15 \tTraining Loss: 0.353989\n",
      "Epoch: 16 \tTraining Loss: 0.304402\n",
      "Epoch: 17 \tTraining Loss: 0.251193\n",
      "Epoch: 18 \tTraining Loss: 0.204606\n",
      "Epoch: 19 \tTraining Loss: 0.166133\n",
      "Epoch: 20 \tTraining Loss: 0.135340\n",
      "Epoch: 21 \tTraining Loss: 0.111057\n",
      "Epoch: 22 \tTraining Loss: 0.091794\n",
      "Epoch: 23 \tTraining Loss: 0.076911\n",
      "Epoch: 24 \tTraining Loss: 0.065156\n",
      "Epoch: 25 \tTraining Loss: 0.055372\n",
      "Epoch: 26 \tTraining Loss: 0.047747\n",
      "Epoch: 27 \tTraining Loss: 0.041216\n",
      "Epoch: 28 \tTraining Loss: 0.036157\n",
      "Epoch: 29 \tTraining Loss: 0.031723\n",
      "Epoch: 30 \tTraining Loss: 0.028192\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 0.481212\n",
      "Epoch: 2 \tTraining Loss: 0.402586\n",
      "Epoch: 3 \tTraining Loss: 0.324561\n",
      "Epoch: 4 \tTraining Loss: 0.251329\n",
      "Epoch: 5 \tTraining Loss: 0.191618\n",
      "Epoch: 6 \tTraining Loss: 0.144514\n",
      "Epoch: 7 \tTraining Loss: 0.109548\n",
      "Epoch: 8 \tTraining Loss: 0.085485\n",
      "Epoch: 9 \tTraining Loss: 0.068374\n",
      "Epoch: 10 \tTraining Loss: 0.055222\n",
      "Epoch: 11 \tTraining Loss: 0.045025\n",
      "Epoch: 12 \tTraining Loss: 0.037527\n",
      "Epoch: 13 \tTraining Loss: 0.031492\n",
      "Epoch: 14 \tTraining Loss: 0.026760\n",
      "Epoch: 15 \tTraining Loss: 0.023180\n",
      "Epoch: 16 \tTraining Loss: 0.019838\n",
      "Epoch: 17 \tTraining Loss: 0.017327\n",
      "Epoch: 18 \tTraining Loss: 0.015210\n",
      "Epoch: 19 \tTraining Loss: 0.013430\n",
      "Epoch: 20 \tTraining Loss: 0.011868\n",
      "Epoch: 21 \tTraining Loss: 0.010617\n",
      "Epoch: 22 \tTraining Loss: 0.009507\n",
      "Epoch: 23 \tTraining Loss: 0.008535\n",
      "Epoch: 24 \tTraining Loss: 0.007705\n",
      "Epoch: 25 \tTraining Loss: 0.007042\n",
      "Epoch: 26 \tTraining Loss: 0.006362\n",
      "Epoch: 27 \tTraining Loss: 0.005819\n",
      "Epoch: 28 \tTraining Loss: 0.005347\n",
      "Epoch: 29 \tTraining Loss: 0.004854\n",
      "Epoch: 30 \tTraining Loss: 0.004439\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 0.809062\n",
      "Epoch: 2 \tTraining Loss: 0.506932\n",
      "Epoch: 3 \tTraining Loss: 0.390665\n",
      "Epoch: 4 \tTraining Loss: 0.297273\n",
      "Epoch: 5 \tTraining Loss: 0.223589\n",
      "Epoch: 6 \tTraining Loss: 0.170443\n",
      "Epoch: 7 \tTraining Loss: 0.130832\n",
      "Epoch: 8 \tTraining Loss: 0.103084\n",
      "Epoch: 9 \tTraining Loss: 0.082306\n",
      "Epoch: 10 \tTraining Loss: 0.066278\n",
      "Epoch: 11 \tTraining Loss: 0.054270\n",
      "Epoch: 12 \tTraining Loss: 0.045156\n",
      "Epoch: 13 \tTraining Loss: 0.038117\n",
      "Epoch: 14 \tTraining Loss: 0.032287\n",
      "Epoch: 15 \tTraining Loss: 0.027619\n",
      "Epoch: 16 \tTraining Loss: 0.024170\n",
      "Epoch: 17 \tTraining Loss: 0.020756\n",
      "Epoch: 18 \tTraining Loss: 0.018346\n",
      "Epoch: 19 \tTraining Loss: 0.016273\n",
      "Epoch: 20 \tTraining Loss: 0.014391\n",
      "Epoch: 21 \tTraining Loss: 0.012744\n",
      "Epoch: 22 \tTraining Loss: 0.011434\n",
      "Epoch: 23 \tTraining Loss: 0.010285\n",
      "Epoch: 24 \tTraining Loss: 0.009278\n",
      "Epoch: 25 \tTraining Loss: 0.008419\n",
      "Epoch: 26 \tTraining Loss: 0.007684\n",
      "Epoch: 27 \tTraining Loss: 0.006945\n",
      "Epoch: 28 \tTraining Loss: 0.006329\n",
      "Epoch: 29 \tTraining Loss: 0.005870\n",
      "Epoch: 30 \tTraining Loss: 0.005328\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 1.468649\n",
      "Epoch: 2 \tTraining Loss: 0.969546\n",
      "Epoch: 3 \tTraining Loss: 0.775591\n",
      "Epoch: 4 \tTraining Loss: 0.683545\n",
      "Epoch: 5 \tTraining Loss: 0.622123\n",
      "Epoch: 6 \tTraining Loss: 0.556294\n",
      "Epoch: 7 \tTraining Loss: 0.484724\n",
      "Epoch: 8 \tTraining Loss: 0.412540\n",
      "Epoch: 9 \tTraining Loss: 0.342935\n",
      "Epoch: 10 \tTraining Loss: 0.279285\n",
      "Epoch: 11 \tTraining Loss: 0.226894\n",
      "Epoch: 12 \tTraining Loss: 0.184144\n",
      "Epoch: 13 \tTraining Loss: 0.150633\n",
      "Epoch: 14 \tTraining Loss: 0.124222\n",
      "Epoch: 15 \tTraining Loss: 0.103476\n",
      "Epoch: 16 \tTraining Loss: 0.086906\n",
      "Epoch: 17 \tTraining Loss: 0.073935\n",
      "Epoch: 18 \tTraining Loss: 0.062956\n",
      "Epoch: 19 \tTraining Loss: 0.054598\n",
      "Epoch: 20 \tTraining Loss: 0.047353\n",
      "Epoch: 21 \tTraining Loss: 0.041244\n",
      "Epoch: 22 \tTraining Loss: 0.036457\n",
      "Epoch: 23 \tTraining Loss: 0.032151\n",
      "Epoch: 24 \tTraining Loss: 0.028536\n",
      "Epoch: 25 \tTraining Loss: 0.025583\n",
      "Epoch: 26 \tTraining Loss: 0.023028\n",
      "Epoch: 27 \tTraining Loss: 0.020721\n",
      "Epoch: 28 \tTraining Loss: 0.018702\n",
      "Epoch: 29 \tTraining Loss: 0.016948\n",
      "Epoch: 30 \tTraining Loss: 0.015376\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 0.611703\n",
      "Epoch: 2 \tTraining Loss: 0.554827\n",
      "Epoch: 3 \tTraining Loss: 0.515490\n",
      "Epoch: 4 \tTraining Loss: 0.471678\n",
      "Epoch: 5 \tTraining Loss: 0.430101\n",
      "Epoch: 6 \tTraining Loss: 0.387191\n",
      "Epoch: 7 \tTraining Loss: 0.347678\n",
      "Epoch: 8 \tTraining Loss: 0.307482\n",
      "Epoch: 9 \tTraining Loss: 0.268840\n",
      "Epoch: 10 \tTraining Loss: 0.233071\n",
      "Epoch: 11 \tTraining Loss: 0.201850\n",
      "Epoch: 12 \tTraining Loss: 0.175094\n",
      "Epoch: 13 \tTraining Loss: 0.151448\n",
      "Epoch: 14 \tTraining Loss: 0.131925\n",
      "Epoch: 15 \tTraining Loss: 0.114891\n",
      "Epoch: 16 \tTraining Loss: 0.100139\n",
      "Epoch: 17 \tTraining Loss: 0.087959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \tTraining Loss: 0.077830\n",
      "Epoch: 19 \tTraining Loss: 0.068767\n",
      "Epoch: 20 \tTraining Loss: 0.061140\n",
      "Epoch: 21 \tTraining Loss: 0.055031\n",
      "Epoch: 22 \tTraining Loss: 0.048901\n",
      "Epoch: 23 \tTraining Loss: 0.044021\n",
      "Epoch: 24 \tTraining Loss: 0.039861\n",
      "Epoch: 25 \tTraining Loss: 0.035645\n",
      "Epoch: 26 \tTraining Loss: 0.032456\n",
      "Epoch: 27 \tTraining Loss: 0.029232\n",
      "Epoch: 28 \tTraining Loss: 0.026580\n",
      "Epoch: 29 \tTraining Loss: 0.024254\n",
      "Epoch: 30 \tTraining Loss: 0.022205\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 0.537520\n",
      "Epoch: 2 \tTraining Loss: 0.409023\n",
      "Epoch: 3 \tTraining Loss: 0.316608\n",
      "Epoch: 4 \tTraining Loss: 0.238788\n",
      "Epoch: 5 \tTraining Loss: 0.178978\n",
      "Epoch: 6 \tTraining Loss: 0.132881\n",
      "Epoch: 7 \tTraining Loss: 0.101435\n",
      "Epoch: 8 \tTraining Loss: 0.078288\n",
      "Epoch: 9 \tTraining Loss: 0.061510\n",
      "Epoch: 10 \tTraining Loss: 0.049684\n",
      "Epoch: 11 \tTraining Loss: 0.040390\n",
      "Epoch: 12 \tTraining Loss: 0.033722\n",
      "Epoch: 13 \tTraining Loss: 0.028161\n",
      "Epoch: 14 \tTraining Loss: 0.023917\n",
      "Epoch: 15 \tTraining Loss: 0.020449\n",
      "Epoch: 16 \tTraining Loss: 0.017777\n",
      "Epoch: 17 \tTraining Loss: 0.015461\n",
      "Epoch: 18 \tTraining Loss: 0.013570\n",
      "Epoch: 19 \tTraining Loss: 0.011949\n",
      "Epoch: 20 \tTraining Loss: 0.010652\n",
      "Epoch: 21 \tTraining Loss: 0.009448\n",
      "Epoch: 22 \tTraining Loss: 0.008588\n",
      "Epoch: 23 \tTraining Loss: 0.007696\n",
      "Epoch: 24 \tTraining Loss: 0.006884\n",
      "Epoch: 25 \tTraining Loss: 0.006229\n",
      "Epoch: 26 \tTraining Loss: 0.005685\n",
      "Epoch: 27 \tTraining Loss: 0.005199\n",
      "Epoch: 28 \tTraining Loss: 0.004724\n",
      "Epoch: 29 \tTraining Loss: 0.004306\n",
      "Epoch: 30 \tTraining Loss: 0.003960\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Epoch: 1 \tTraining Loss: 0.488941\n",
      "Epoch: 2 \tTraining Loss: 0.412824\n",
      "Epoch: 3 \tTraining Loss: 0.336397\n",
      "Epoch: 4 \tTraining Loss: 0.269046\n",
      "Epoch: 5 \tTraining Loss: 0.210511\n",
      "Epoch: 6 \tTraining Loss: 0.163939\n",
      "Epoch: 7 \tTraining Loss: 0.127610\n",
      "Epoch: 8 \tTraining Loss: 0.100684\n",
      "Epoch: 9 \tTraining Loss: 0.080870\n",
      "Epoch: 10 \tTraining Loss: 0.064902\n",
      "Epoch: 11 \tTraining Loss: 0.053356\n",
      "Epoch: 12 \tTraining Loss: 0.044048\n",
      "Epoch: 13 \tTraining Loss: 0.037291\n",
      "Epoch: 14 \tTraining Loss: 0.031515\n",
      "Epoch: 15 \tTraining Loss: 0.027008\n",
      "Epoch: 16 \tTraining Loss: 0.023421\n",
      "Epoch: 17 \tTraining Loss: 0.020391\n",
      "Epoch: 18 \tTraining Loss: 0.018070\n",
      "Epoch: 19 \tTraining Loss: 0.015873\n",
      "Epoch: 20 \tTraining Loss: 0.014209\n",
      "Epoch: 21 \tTraining Loss: 0.012634\n",
      "Epoch: 22 \tTraining Loss: 0.011311\n",
      "Epoch: 23 \tTraining Loss: 0.010184\n",
      "Epoch: 24 \tTraining Loss: 0.009135\n",
      "Epoch: 25 \tTraining Loss: 0.008295\n",
      "Epoch: 26 \tTraining Loss: 0.007534\n",
      "Epoch: 27 \tTraining Loss: 0.006889\n",
      "Epoch: 28 \tTraining Loss: 0.006248\n",
      "Epoch: 29 \tTraining Loss: 0.005727\n",
      "Epoch: 30 \tTraining Loss: 0.005253\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "{1: 100.0, 2: 100.0, 3: 100.0, 4: 100.0, 5: 100.0, 6: 100.0, 7: 100.0, 8: 100.0, 9: 100.0, 10: 100.0}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdeXhU1eH/8fedJZmZ7Mtk31gCJCQhQFgkoGwqWteKVnBttfZbl/5stZut2mpdutiqba22anG3am3rhgvKjiwJgZCFJWQhK0km22QmyWzn98cNECCEBAZCJuf1PPNMZu6Ze0+IfnJyzrnnKEIIJEmSJN+gGe4KSJIkSd4jQ12SJMmHyFCXJEnyITLUJUmSfIgMdUmSJB+iG64LR0ZGipSUlOG6vCRJ0oiUn5/fLIQwn+j4sIV6SkoKeXl5w3V5SZKkEUlRlKqBjsvuF0mSJB8iQ12SJMmHyFCXJEnyITLUJUmSfIgMdUmSJB8iQ12SJMmHyFCXJEnyISMu1J0NNtpXVuDpdg13VSRJks45Iy7UXS3dWNfW4Gy0D3dVJEmSzjkjLtR1USYAXE1dw1wTSZKkc8/IC/UwA2gVXE2ypS5JknSsERfqilZBF2HA2Shb6pIkSccacaEOoDObZEtdkiSpHyMy1PVmE66WboTbM9xVkSRJOqecNNQVRXlZUZRGRVGKTlJuhqIobkVRlnqvev3TmY3gFrhaus/0pSRJkkaUwbTUVwBLBiqgKIoW+C3wmRfqdFJ6OQNGkiSpXycNdSHEOqDlJMXuAf4NNHqjUiejMxsBZL+6JEnSMU67T11RlHjgauD506/O4GgMOjRBfnIGjCRJ0jG8MVD6NPBTIYT7ZAUVRblDUZQ8RVHympqaTuuierNRttQlSZKO4Y1QzwHeVhSlElgKPKcoylX9FRRC/F0IkSOEyDGbT7hv6qDozEacTV0IIU7rPJIkSb7ktDeeFkKMOfS1oigrgI+EEP893fOejM5sQnS58NicaAP9zvTlJEmSRoSThrqiKG8B84FIRVFqgIcBPYAQ4qz1ox/r8AyYxi4Z6pIkSb1OGupCiGWDPZkQ4tbTqs0QHJoB42y24z825GxdVpIk6Zw2Iu8oBdCG+KPoNbjkDBhJkqTDRmyoKxoFXaScASNJktTXiA11UNdWd8q7SiVJkg4b0aGuNxtxt3YjnHJhL0mSJBjhoa4zG0GAyyJb65IkSTDiQ12d1ij3K5UkSVKN7FCPPLSwl2ypS5IkwQgPdY2fFm2ov5wBI0mS1GtEhzrIGTCSJEl9jfhQP7Rao1zYS5IkyQdCXWc2IRwe3B2O4a6KJEnSsPOBUO8dLJUzYCRJkkZ+qOvNcr9SSZKkQ0Z8qGuC9Cj+WpxyBowkSdLID3VFUdBFmWRLXZIkCR8IdeidASP71CVJknwj1HVmE+4OB54e13BXRZIkaVj5RKjrzXK5AEmSJBhEqCuK8rKiKI2KohSd4PgNiqIU9j42KYoyxfvVHJguSs6AkSRJgsG11FcASwY4XgFcIITIAh4F/u6Feg2JLtwAGuQMGEmSRr3BbDy9TlGUlAGOb+rzcjOQcPrVGhpFp0EXbpQtdUmSRj1v96nfBqz08jkHRWc2ynXVJUka9U7aUh8sRVEWoIb63AHK3AHcAZCUlOStSwPqDJjuva0Ij0DRKF49tyRJ0kjhlZa6oihZwIvAlUIIy4nKCSH+LoTIEULkmM1mb1z6ML3ZCG6Bu7Xbq+eVJEkaSU471BVFSQLeB24SQuw9/SqdmkMzYOTa6pIkjWYn7X5RFOUtYD4QqShKDfAwoAcQQjwPPAREAM8pigLgEkLknKkKn8iRre3sMCn8bF9ekiTpnDCY2S/LTnL8duB2r9XoFGkD9GgC9HIGjCRJo5pP3FF6iJwBI0nSaOdToa6XqzVKkjTK+VSo6yKNeGxO3DbncFdFkiRpWPhWqB9aA6ZZttYlSRqdfCrU9XK/UkmSRjmfCnVtmAG0ipyrLknSqOVToa5oFHSRRnWuuiRJ0ijkU6EOcgaMJEmjm8+Fus5sxNXShXB5hrsqkiRJZ53PhbrebAIPuFrkwl6SJI0+PhfqOjkDRpKkUcxnQ13OgJEkaTTyuVDX+OvQBvvJGTCSJI1KIy/UXQ4o+xKEOGERXZRJttQlSRqVRl6oF/4LXv8m1BWcsIjObMTVaEcMEPySJEm+aMSFercujYbtYXi2v3PCMnqzCdHjxmOVC3tJkjS6jLhQd7XZad1rxL76f+Dpfy76kcFS2a8uSdLoMuJC3ZSTg6LTYtvfCdWb+y1zeLVG2a8uSdIoc9JQVxTlZUVRGhVFKTrBcUVRlGcVRSlTFKVQUZRp3q/mERqTCeO0qdgOGqDo3/2W0Qb7ofhp5AwYSZJGncG01FcASwY4fgmQ2vu4A/jb6VdrYIHnX0BPmw7ntv+C23XccUVR0JnlDBhJkkafk4a6EGId0DJAkSuBV4VqMxCqKEqstyrYn4DcXABsFTaoWNtvmUMzYCRJkkYTb/SpxwPVfV7X9L53HEVR7lAUJU9RlLympqZTvqD/xIloIyKwNQaesAtGbzbhbuvB43Cf8nUkSZJGGm+EutLPe/1OEBdC/F0IkSOEyDGbzad+QY2GgNw52BpNiJIPwdVzXJnDa8DIre0kSRpFvBHqNUBin9cJQJ0XzjugwLlzcducdDd0Qdmq447rD8+AkV0wkiSNHt4I9Q+Am3tnwcwG2oUQ9V4474ACzjsPAJslDHa9d9xxXYQRFHA2ypa6JEmjh+5kBRRFeQuYD0QqilIDPAzoAYQQzwOfAJcCZYAd+PaZqmxfOrMZ/0mTsLU1Ern3U3DYwC/gSL31GrThBpy1nWejOpIkSeeEk4a6EGLZSY4L4C6v1WgIAufmYlmxAk92F5o9KyFz6VHHjRmRdK6vwd3hQBvsNxxVlCRJOqtG3B2lfQXk5oLLjc0a2+8smMAZMeABW17DMNROkiTp7BvRoW6cPh3FYMDWPQ72fQFdrUcd10Ua8R8fim1rA8IjV2yUJMn3jehQ1/j5YZo5A1u5DTxOKP3ouDIBM2Nwt/XQs6+1nzNIkiT5lhEd6gCBubk4ahpwaFP67YIxpkegCdTTuUV2wUiS5PtGfKgHzJ0LgE1kq0sGdDYedVzRaQiYHk33bgvujuNvUpIkSfIlIz7U/caORRcTg61WA8IDJf87rkzAoQHTbQeHoYaSJElnz4gPdUVRCJibi62gBBGZ1v+NSIcGTLfJAVNJknzbiA91UPvVPR0ddAXMUzfOaKs+rsyhAdPuvXLAVJIk3+UToW6aPRsUBVtziPpG8fvHlTk0YGrbKgdMJUnyXT4R6rqwMAyZmdi2l0DctH5nwSg6DQE5vQOm7XLAVJIk3+QToQ4QkDuHrsJC3OMuh/qd0Fx2fJnDA6aytS5Jkm/ymVAPnDsX3G5snfGA0m9rXRdhxD81FNu2g3LAVJIkn+QzoW7MykITEIAtvwSSc6HoPRDHB3fAzFjc7XLAVJIk3+Qzoa7o9ZjOm41twwbE5KuheS8cLDqunDE9XB0w3XLGl3yXJEk663wm1EGd2uisq8MROA0Ubf8DploNATkxdO9uwSUHTCVJ8jE+FeoBubkA2LaXwrgFaqj31wUzIxoE2OWAqSRJPsanQt0vKQl9UhK2jRsh4xpoOwA1eceVOzJgKu8wlSTJt/hUqIM6tdG+ZQti3EWg9Yedb/ZbLnBWLO52B917Ws5yDSVJks6cQYW6oihLFEXZoyhKmaIoP+vneJKiKKsVRSlQFKVQUZRLvV/VwQmcOxeP3Y69tByyl0H+K3Cw+LhyhrRwNEF6bHJJXkmSfMhJQ11RFC3wV+ASIB1YpihK+jHFfgm8I4SYClwPPOftih5iqalm3ZsrcDkc/R43zZoFWi22DRth0cNgDIWPfggez1HlDg+Y7mnB1SYHTCVJ8g2DaanPBMqEEOVCCAfwNnDlMWUEENz7dQhQ570qHq3tYD3b/vce9ft293tcGxiIMTtb7Vc3hcOFj0L1Ftjx+nFlA2bEAPIOU0mSfMdgQj0e6LvsYU3ve339CrhRUZQa4BPgnv5OpCjKHYqi5CmKktfU1HQK1YWEtAwUjYYDxYUnLBM4N5fukhJcLS2QvVy9GemLh8DWfFQ5XbgB/9Qw7NsaEG45YCpJ0sg3mFBX+nnv2ARcBqwQQiQAlwKvKYpy3LmFEH8XQuQIIXLMZvPQawv4m0xEjx3PgaITh3pAbi4IgW3T16Ao8I2noMeqBvsxAmfG4O6QA6aSJPmGwYR6DZDY53UCx3ev3Aa8AyCE+BowAJHeqGB/kiZn0VC2B0d3V7/HDZMnow0JUbtgAKLSYM49sOMNqNx4dNlDA6ZySV5JknzAYEJ9G5CqKMoYRVH8UAdCPzimzAFgEYCiKGmooX5q/SuDkJgxBY/bTe3ukn6PK1otpjnnYdu4EXHo5qPzfwIhSfDxj8Dl6FP2yICpo8Z6pqosSZJ0Vpw01IUQLuBu4DOgFHWWS7GiKI8oinJFb7H7gO8qirITeAu4VYh+buX0kviJaWi0Og4U7TxhmcDcXFyNjfTs26e+4WeCS38PTbth81+PKRuHNsSf5ldL5FrrkiSNaIOapy6E+EQIMUEIMU4I8Vjvew8JIT7o/bpECJErhJgihMgWQnx+Jiut9zcQN2ES1QMMlh5eMmDjpiNvTlwCky6DNb+F1qrDb2sD/Yi8dTKix03zK8V4HO4zVndJkqQzacTeUZo4OYuDFfvp7uzs97g+Nhb/SZNoee1VXK19ltld8iQoGlj5k6PWhdHHBBC+bBLOehstb++RywdIkjQijdhQT8rIAiGoLt11wjKxjz6Ku9lC3X33I9y9re/QRJj/M9j7Kez++KjyxknhhFw2lu4SCx2fVZ7B2kuSJJ0ZIzbUY1MnovPzp3qAqY3GzAxiHnoQ26ZNND375yMHZn8foibDyp9Cz9Et/cA5cQTMjsW6tgZbnpwRI0nSyDJiQ12r0xM/KX3AwVKA0KVLCb12KZYXXsD65Ze9H9bDZX+EjhpY++RR5RVFIfTysfinhtL6fhnd+9vO1LcgSZLkdSM21AGSMqZgqTmArW3gremif/lLDJMnU/fTn+GorOz98GyYdjN8/Rw0HL1DkqLVELE8DV2kgZY3SnE29z8fXpIk6VwzskN9chYA1SUn7lcH0Pj7k/DsMyg6HTX3/ACP3a4eWPxrdcGvj3903IJfGqOOyFsmA2BZUYzH7vT+NyBJkuRlIzrUo8aMw89oGrBf/RB9fDxxT/2BnrIy6h98SL0pqe+CXwWvHfcZXYSRiJvTcbV2Y3m9FOH29HNmSZKkc8eIDnWNVktCegYHigfuVz8kMDcX8//7f3R8/DGtr/Wu2nhowa9VD0Pj8Ss/+qeEEHZNKj3l7bT9dz9n8J4qSZKk0zaiQx0gafIU2hrq6WhuHFT5iDu+S+DChRz83e+w5+erC35d/gxo9PDShVD25XGfCZgWTdCCRGzbGuhcX+vtb0GSJMlrRn6oZ/T2qxcP3K9+iKLREPfbJ9HHx1Fz7704GxshMhW++yWEJMIb18K2F4/7XPCFyRgzI2lfWYG98IwtayNJknRaRnyoRyYmYwwKPunUxr60QUEkPPtnPJ02an/4I4TTCaFJcNtnMH4xfHyfOofd7Tr8GUWjEHbtBPwSg2h5czcdq6rkXaeSJJ1zRnyoKxoNiZOzOFBcOKT+bsPECcQ++ihd+fk0/uEP6pv+QbDsLZh9F2x5Ht66Hro7Dn9G46fF/N1MTFOj6Fh1AMurJXi6XCe4giRJ0tk34kMd1C6YTkszbQ1D20Uv5LJvEHbTTbS88irtH/cuGaDRwpLH4Rt/hP1fwcsXH7X4l6LXEnbdBEKvHEf33lYa/1KAs8HmzW9HkiTplI24UN9a0cJ1L3zNj9/dyV9Xl/FRYR1dkWMBBtwN6USif/JjjNOmUf+LX9L4xz/hau7d8m7GbXDjv6G9Fl5cBNVbD39GURQCz4vDfEcmHoebxr/uwL5T9rNLkjT8lOGaopeTkyPy8vKG/LlNZc38adVeqix2Gq29a58LwberX6M5II7q7KUkR5hIjgggOdxEYriJhDAj0cEGtJr+duYDV3MzDY/+Buvnn6Po9YRcfTUR3/k2fsnJ0LQX3rwOOurgqucgc+lRn3V3OLC8UYqjqoPAefGELBmDou3/OpIkSadLUZR8IUTOCY+PtFDvy9bj4kCLnSqLnT3/egFHVQk75t1LVUsXdW1d9B3H1GkUYkIMxIcaiQ8zkhBqJCHMRHyYkfhQI7GhBpSaaiwv/5P2//wH4XIRdNFFRNx+O8axsfCvG+HAJpj/c7jgp+pUyF7C5aHt43JsX9fjPzaE8OWT0Ab6ndb3JkmS1B+fDvW+ilZ/wWfPP8Mtv/8LkUkpOFweqlvt1LR2UdvaRU2rndo29evati4OdnRz7OQVc5A/caFGxmu7mVv4JeO//gxdlx1P9nRCbr2ZOPu/0RS+DSnzIPdeGL/oqHC35R+k9T9laAP0RNyYhl9ikNe+P0mSJBhFod7R1Mg/7v4OC269g2mXXHHS8k63h4b2bjX029TQr2/rpq5dbeXXtXWj2G1cUvk1V+9fT0R3B/tD47FmRHFRwmYisNBoGENx8k20jbuKyLBgooIMRHS6cL63D7fVQdhV4wmYEeO171GSJMkroa4oyhLgGUALvCiEeLKfMtcBvwIEsFMIsXygc55OqLtcLnQ63XHvv/iD24lMTOGqH//ylM7blxCC9i4ntW1d1Dd14Pj0EyI/fpegxlrshgAssREkRtWSGltLq38gr7ku4nX3YloIJlKj4dcaE1NcGvIi9ezPCCUxIoDEcBNJESZiBujflyRJGshph7qiKFpgL3AhUANsA5YJIUr6lEkF3gEWCiFaFUWJEkIMeN/+qYZ6ZWUl77//PldffTVjxow56tjnLzzL3s0bufOlN9FotEM+98kIj4fONWuxfv45nevX47ZYQFHwi/EnOKIRY5ygOmMJ6yKuY48zmpwKO+e3e9iEk4fp4tACvnqtQnyoUQ353kdyhIkxkYEkR5gw6L1fd0mSfMPJQv345u7xZgJlQojy3hO+DVwJlPQp813gr0KIVoCTBfrpMBgM6PV6XnnlFc4//3wuuOACtFo1BBMzprDrq89prCgnZlyq16+taDQELVxA0MIFCI+H7pJSOtetxbZ2Hc2FPVAk0K7dyOWxX3L9tEkEfusHdNmzmfNhOZ+HB1OzKJ4Kh5MDLXaqW+1Ut9j5eFc9bX2W9VUUiAsxMiYygJRINejHRgaQEhlAQpgRvXbEzUKVJOksGkxLfSmwRAhxe+/rm4BZQoi7+5T5L2prPhe1i+ZXQohP+znXHcAdAElJSdOrqqqOLTIoDoeDTz75hB07dpCYmMg111xDaGgotrZWnv/eTcxbfiszr1x68hN5kau1FduGDXR+9QW2detw29Tpln4hHgxpcxCxN6L46Ym4JQPD2PCjPtve5eSAxU55cyeVzXYqmjupaLZR3mzD2n3kjlWdRiGxt1WfEhFw1HNCmAk/nQx8SfJ13uh+uRa4+JhQnymEuKdPmY8AJ3AdkACsBzKEECfcC84bA6W7du3iww8/RKPRcMUVV5Cens6K++4kKCKSax545LTOfTqE2013QR62j16na2cBXZXNCF0cxtl3oxhCcB94D0OyFsN5CzFOycYvJQVFc3wgCyFosTmotNgob7JRabFR0WyjyqJO4+zsORL4GgXiQo1Hhf1YcwATooOIDzWikX34kuQTvNH9UgMk9nmdABx7P34NsFkI4QQqFEXZA6Si9r+fMZmZmcTHx/Pee+/xzjvvkJOTQ1xaJqXrVuF2OdHq9Gfy8iekaLUYc2ZhzJkFqCHv2rUG25cfY6vJRhmzHPvu/9D6wS8A0Bj9MUxOwzh9JsYpUzBmZaGLjERRFCIC/YkI9Gd68tGteyEEFpujN+BtVPZ5/mRXPa19unRMflpSowKZEB2kPmKCmBAdSEywAUWRYS9JvmQwLXUdatfKIqAWNaiXCyGK+5RZgjp4eouiKJFAAZAthLCc6LzenNLocrn46quv2LRpEyGBgTh3buaGXzxC/KR0r5zfm4TLQ+s7RdgL2/EP3IfuwFN011nptujpbtODUENWbw7DMGWKGvRZWRjS09EYjYO+TrvdSVmTlb0HO9nTYGVfo5U9DZ00d/YcLhNk0B0O+vTYINJig5kUG0yg/2B+10uSNBy8NaXxUuBp1P7yl4UQjymK8giQJ4T4QFGbe08BSwA38JgQ4u2BzunteeoAZWVlvP/++9g7O0lPjOXa2//vnGyJCiGwflVNxxdV+CUHE3GxQGvJw1OVR/fO7XSV1dHVrKPbosdp7w1YjYJ/SjzG7KkYMqfiP2EC/qmpaIODh3TtFpuDvQet7DtoZc/BI6Hf3nWkZZ8UbiI9Npi02GDSesM+Icx4Tv5bStJoM2puPjrEarXy3O+epEurJz09ncsvvxzjEFq4Z5O9sImWd/aiDfYj8tbJ6KNM6gGHHQ4WQV0Brj1b6Nq5g66KJroserpb9HicR/rfdZEh+KemYpg8Bf+JE9WwHzMGxW/wyxQIIahv76a0voPS+g5K6jsorbdSabFx6D+PIIOOtNhgMuJCmJIYQlZCKCkRJhn0knSWjbpQB1j96ots2boVR1QC/v7+zJkzh5kzZ+Lv739Grnc6eg50YHm1BDyCiFsn4590gpZ3Tyc0FCJqC3DtyaO7tIieqgZ62jT0tOnpserA0xuwWg3+iXEYMrMxZGer3TcTJw4p6EFdW2fPQSsldX3DvoNup7oBd7BBR1ZCKFkJIUxJDGVKQigxIYbT+eeQJOkkfDLUPR4nGs2JB0HLC7bxnyd/zYK776ekqoaysjJMJhO5ubnMmDEDvyGG25nmsnTR9FIRnk4HETemY5gQNsgP9kDTbqgvRNTspKe0gJ6y/fQ0u+hp19PV4oe7W23VKzot/hPGYpx2ZDBWn5Q05Ja2y+1h78FOCmva2FnTzs7qNvYctOLuXUgnKsifrIRQpiSEkBEfwuS4YKKCZdBLkrf4XKg3N69m954HmZHzb/z9o/st4+iy85fvXM/MK5cy9/qbqa6uZs2aNezfv5+AgAByc3PJyck5p8LdbXXQ/HIRzkY74d+aiCnLfGon8nigrVIN+roCXKVb6SreTddBF90WNeiFWw1yTYARY0YaxpzZGKdNwzglG21gwJAv2e10U1zXQWFNG4W9QV/efGTjEHOQPxlxwUyOCyEjXn2WffSSdGp8LtTt9iq2bF2COfIiMjKeOWG5Nx+8H4Rg+W+eOvzegQMHWL16NRUVFQQGBjJ37lymT5+OXj88Ux+P5ely0fxKMY6qDkKvHEfg7DgvndgDLfuhdjuiOo+eXVvp2l1BdxN0Wfzo6dCps24UBcPYeIwzZmGaPRfjtGnoo6JO6ZId3U5K6zooruugqK6dkroO9jV2Hm7RBxt0h0N+enI4c8ZHEGw4N34OknQu87lQBygvf4aKymeZmv0q4eG5/ZbZ8PZrbP3fu9z10tv4m0xHHausrGTNmjVUVlYSGBjIvHnzmDZt2jkR7sLpxvLmbrpLWwi+MJmghYlnpkXrdkJjKdRtx71/K13b8+jaV4+9SU+XRY9wq902+qhgTFkZGHMXYJo1B78xY065Pt1ON3sarBTVtVNc10FxbTulDVYcLg9ajcLUxFDmpZqZNyGSKQmhctEzSeqHT4a6293Nli2XoGj0zJr5ERrN8d0oB4p28u6jv+Dqnz7M2Gkz+j1PRUUFq1ev5sCBAwQHB7Nw4UKysrLQ9HN359kk3B5a/70P+/ZGAufEEXLZWJSzEXAOm9ptU7WV7vwN2AtL6Kq2YW/yw92jrq+jNekxpo3BNCsX49zFGDIy0JxGN5bT7aHgQBvr9jaxfl8ThbXtCKG25OemRjIv1cz5E8zEh56bM5gk6WzzyVAHtW99Z+HtjBv7Y1JS/u+4405HD3/9zvVkLV7Cwlu/d8LzCCGoqKhg1apV1NXVERsby8UXX0xKSsop180bhEfQ/kkFnRtqMWWbCbt2AspwLObV2YSozcdRsIaubVux7zlA10EFh1WdP69oFQxjozFNm47x/Isw5cxCGxJyypdrtTnYUNbM+n1NrNvbTENHNwBjzQGcn2pmzrgIZo+TXTXS6OVzoe4RHrbUb+G8uPMoLPw/LC0bOG/25xgMx/c/f/T0bynL28xNTz5LREJiP2frc16Ph6KiIlatWkVHRwdpaWksXryYiIiIIdfRW4QQWNfW0PFpJYaJYYTfkIbGb5iX5fV4oHkPrqKvsG9aTdeuPdir7XS3Hrkb1j8mCOOUyQRddi0BFywe8lTKQ4QQlDV2sm5fM+v2NrGlwkK304NGgayEUHLHR5A7LpJpyWFyuWJp1PC5UP/Xnvf4ZeFn/Ch1OrdMvIwtWy4mIuJ8sjL/dlxZW1srK+67k9CYWJY98ns02pP/j+9wOPj666/ZsGEDbrebWbNmcf755w/rDUydW+tp+08ZfknBRN6SjsZ0jrVSbc14yjbQtfFTurbvwL6/CXuTDuHSoPFTCMyMJ/jCRQRccQua8NhTvkyPy03BgTY2ljWzsayZnTXtuD0Cf52GnJQw5oyLJHd8JJnxIbI/XvJZPhfqK6oP8rOyeoztH/LdGB3LYkIoL3+KKVNeIjJi/nHld29cy8fP/n7Iy/FarVa++uorCgoKMBqNzJ8/n5ycnMNrt59t9l3NtLy9G12kEfNtmWiDz53pmMdxOfBUbsG28h2s67Zg3dOOx6FB0XoIHGMg6LwpBH7jGrSTFoBhaMsc9GXtdrK1ooWNZRY27W9md4MVUO9+vWCCmUszY5k/0YzJT65lI/kOnwv1Xetq+OmeGraPN2Ds+IRrQ1q4yn8H4GbWzE/Rao++a1QIwYd/fILy7Vu56bfPEpGQNKTrNTQ08Nlnn1FRUUFERAQXXXQREyZMGJY51t1lrVheLUUTqMd8Wwa6iJExeCi6OrF/+jbWTz/Cml+Gq9ONohGYYhwEZ0QRtHgR2swlkDgLdKf+y6rJ2sPX5RY27mtmVelBLDYHBr2G+ROiuCQzhoWTogiSffHSCOdzoe7xCL74Zym38OYAACAASURBVDF/cXeyLdWA0foZNxrzmK/dzpgx9zJ2zD3HfcbW1sqK++8iNCqaZY/+YVDdMH0JIdi7dy+ff/45FouFlJQUFi9eTEJCwpDrf7oc1Vaa/1kEWgXzbZnoY4Z+s9BwEh4PXflbsf7nDaxrN+O0dKJoBcHJdsLTPBim58K4heojYry6FdQpcHsEWytaWFlUz8qiBpqsPfjpNJyfauaSjBgWp0cTYpQBL408PhfqAB63hy/+WcxzLhtbJhowdn7Jz/zfIdXPynmzPsVoPL41vnvTOj5+5nfMvf5mZl193Sld1+12k5eXx9q1a7Hb7aSlpbFo0SIiIyNP6XynynnQRvNLRXgcHiK/PRn/5FPvwhhOQgi6i0toe+t12j/6BNHjwBitEDamhaDELjThiTBugRrwYy4AU/jJT9oPj0eQf6CVlbsaWFlUT317N3qtQu74SC7NiOWiydGEms7h7ixJ6sMnQx3UYP/spWL+4epkU5qRENs6/mj8C5Fhs5g59dXjukeEEHz0pyfZn7+FG594msiklFO+dk9PD19//TWbNm3C6XQydepU5s+fT/AQl8E9Ha6Wbppf2oW7w0HETUNYL+Yc5e7ooP2//6X1jTdxVFWhDTYROiWYsOhy9Lo2QIH4aTDhEph4CURPPqVWvMcj2FnTxsqiBj7ZVU9Naxd6rcK8VDOXZcVyYXq07KKRzmk+G+oAbreHz18s4iWnjQ2TjYxxbOAR/TPEjHuYzOQbjytvb29jxX13EmyOZvlvht4Nc6zOzk7Wr1/Ptm3b0Gg0zJo1i7lz5561mTJeWy/mHCI8Hmybvqb1zTfpXLMGgMDzsgmfEYlJU4hSt10tGJKkhvvESyA595T64oUQ7Kpt56PCej4urKe2rQs/nYb5E8xcNiWORZOiCJAbhkjnGJ8OdegN9n8Us8JhZW2miWmeDdzq+QuTsl4nK/r473vP1xv46OknT6sb5litra2sXr2awsJCDAYDc+fOZdasWWdl2YGj1ou5ejyBM099yuC5xlFTS9u/3qbt3fdwt7XhN24cETddR8hYN0rZZ1C+Glzd4B8M4xfDxEshdTEYh/5Xi8cjKKhu46PCOj4urKfR2oNBr2HRpGguy4plwaQoORdeOif4fKgDuF0ePvtHEa85OlmdZWKW2MTkzhe4LOdvzImbc1z5D//0JGXbNnPjk09jPo1umGM1NDTw5Zdfsm/fPoKCgpg/fz7Z2dlnfBqkx+Gm5Y1Suve0ErwkheD5A99oNdJ4enroWLmSln+uoGfPHvSJiUTc8V1CL70IpXoj7PkE9n4KtiZQtJA8ByZdBmmXQ0j8kK/n9gi2VbbwUWEdK3c1YLE5CPDTcnFGDEunJzB7TITcyFsaNqMi1EEN9k9f2MVbDhursk1MF1voOfg8981+nEvGXHJUWXtHOyvuu5OgiEiW/+YptDrv/oldVVXFF198QU1NDWazmQsvvJDU1NQzOg1SuDy0vLuXrp1NBF6QQMiSFJ9b2lZ4PHSuXk3zc3+ju7gYXVwsEbffTug116DR66E2H/auhN2fQFOp+qGEmZB+BaRdAWHJQ76my+1hc3kLH+6s45Nd9Vh7XCSEGblmWgJLpyeQGG46+UkkyYu8tUfpEuAZ1D1KXxRCPHmCckuBd4EZQogBE/tM7HzkdnpY+cIu3nM382lWBOliB43VT/OzGfdzY/rRfex7t2zkwz8+Qe51NzL7muu9Wg9Q+2t3797NF198QUtLCykpKVx00UXExXlpOd3+rukRtH2wH9vmegJmxBB69fizsxDYWSaEwLZ+Pc3P/Y2uHTvQRUURcdt3CL3uuiObczfvg5L/qY+GQvW9uKlquKdfCRHjhnzdLoebz0saeDevho37mxECZo8N59rpiVySGSNvcpLOitMOdUVRtMBe4EKgBtgGLBNClBxTLgj4GPAD7h6OUAdwOd2sfH4XnwYV8O/kqaSI/XRWP8ZtGTdx77R7j2q9fvT0b9m39WtufOJPmJPHeL0uoE6DzM/PZ82aNdjtdjIzM1m0aBGhoaFn5HpCCDq+qML6VTXGyRGEXz8JRT+8q06eKUII7Js30/zc37Bv24Y2IoKIb99K2LJlaAL6zN9vqYDSD9SAr81X34vOUMM9/UowTxzytWvbuvjP9hrey6+h0mInwE/LpZmxXJuTyIyUMJ/7K0k6d3gj1M8DfiWEuLj39c8BhBBPHFPuaWAVcD9w/3CFOqjB/slzBWwY+z6vBSwlSdOA7cCDXDl2Cb+a8yv0vVvh2TvaeeX+uwgMi2D5Y97vhumru7ubDRs2sHnzZoQQzJo1i3nz5p2xmTLWDbW0f1SOX0owkTefg+vFeJk9L4/m5/6GbdMmtCEhRN75fcJuuAHl2J9pWzWUfqgGfPVm9b3oTMi6FjKWDrkPXghBXlUr7+XV8FFhHTaHm5QIE8tmJnFtTiLhAXL+u+Rd3gj1pcASIcTtva9vAmYJIe7uU2Yq8EshxDWKoqzhBKGuKModwB0ASUlJ06uqqk7hWxocl8PNJ//4ii3jPuRl/1tJ0bVhrfgJ8+Jm8NQFT2HSq32h+7Zu4oOnHmfOtTdw3tJlZ6w+h7S3t/PVV1+xc+dOjEYjF1xwATk5OejOwC8U+84mWt7Zgy7SSOS3M9CFnnsbb3tb144dNP35L9g2bsQweTKxjz6CIT29/8Id9VDyX9j1bm8LXoGUuZC5VG3BD3EWjd3h4tOiBt7eVs3Wihb8tBouzYzhhtnJ5CTL1rvkHd4I9WuBi48J9ZlCiHt6X2uAr4BbhRCVA4V6X2eypX6I0+Fm5cv/Jn/8F/xd+33G+XXRXv5DMsPH89dFfyXMoP5P+/Gzv2fv5g1c9/CTxE9MO6N1OqS+vp7PP/+ciooKwsLCWLhwIZMnT/b6Bh3d+9uwvFqCxl9L5HcyRtyyAqdCCIH1009peOxx3K2thN98M+Z77kZjGmBQ07JfDffCd9St/7R+kHoRZF4LEy4G/dD+otp70MqbWw7w7/warD0uJkYHccPsJK6eGi9vbpJOyxnvflEUJQTYD3T2fiQGaAGuGCjYz0aogxrsn73+PAUpW/mb8v9INbixlt9LvCmc5y98nvjAeLo6rbz5ix/RY7dz4+N/Ith8avtyDpUQgrKyMr744gsaGxsxm80sWLCASZMmeTXcHXWdNP+zGOH0EHlzOv5jT30Ti5HE3d5O4x+eou3dd9HHxRHz618ROG/ewB8SAuoK1IAv+jd0HlTnwaddDlOuh+S5MISfjd3h4sOddby++QC7atsx+Wm5MjueG2YlkRE/On4Oknd5I9R1qAOli4Ba1IHS5UKI4hOUX8M50lI/xOlw88V7v2RHTDV/UX7ERKNCV+V9GDWC5xc/z8TwiVhqq3nrl/cTHGnm+kd/j5/h7K2A6PF4KC4uZs2aNVgsFqKjo1mwYAETJ0702p/srtZuml8uwtXaTfi3JmHKPLvr1Qwne14e9Q89jKO8nOBvfIPon/8M3WDW6/G4oWId7HpPHWjt6YCwFJh6I0xZPuT+953VbbyxpYoPdtbR7fSQnRjKTbOTuWxKLP46eWOTNDjemtJ4KfA06pTGl4UQjymK8giQJ4T44JiyazjHQh3A0ePiq4++w44wF39WfswEkxaqH6DbYeHZhc8yI2YGlTu38/4Tv2Ls9Jlced8DKGd5r1KPx8OuXbtYu3YtLS0txMbGsmDBAq/NcXfbnFheKcZRbSX08nEEzjlz0yvPNR6HA8vf/4HlhRdQTCaif/JjQr75zcH/uzq71AHW7a9C5XpQNDBuEUy7SV2PZgjLFLTbnbxfUMPrm6vY32QjMtCP5TOTuGF2MtHBhlP8DqXRYtTcfDQYPV121q76JgWGSP6su48JJj3Ght9Q37GXJ+c9yUUpF7F95QesXvF3Zl65lHnLbz2r9TvE7XZTWFjI2rVraWtrIz4+ngULFjBu3LjTDnePw03LW7vpLm0haH4iwRcnj6oBvJ79+6l/+GG68vIxzZxJzK9/hf+YIU5nbSmHgjdgx5tgrQNTBGRdrwZ81ODHZIQQbChr5pVNlXy5uxGtonBJZiy3zklhWlLoqPq5SIMnQ/0YNutBNm28gnyRzl8N/4/xAX5EW56mpHEzP5z+Q25Jv4UvX3yOwi8/5ZK7fkT6+QvPeh0Pcbvd7Nixg3Xr1tHe3k5iYiILFixg7Nixp3Ve4Ra0/a8M29YGTNOiCLsmdXg2tR4mwuOh7b33aPzDU4jubiK//39E3Hbb0PdS9bhh/1dq633PSvA4IT5HDfeMpeAfOOhTVVlsvPp1Fe/kVWPtdpGVEMKtc1L4RpbsmpGOJkO9H60txeTnX0e+/Xz+Gvw9xpr8yeh6nbVVH3DV+Kv4Rc4DfPDkI9TtLeW6h58gbsLZmRFzIi6Xi4KCAtatW4fVaiUlJYWFCxeSlDS0XZz6EkJg/fIAHasO4D8hjIjlk9AYRtcdka6mJhoefxzryk/xT00l9tFHMGZnn9rJbM1Q+C/Y/pq6RIFfEGRdBznfhpjMwZ+mx8X7BbWs2Fghu2akfslQP4GG+lUUlfwf25qv5vmoG0g2+XOJfgNv7nqW6dHTeSLnET555FEc3V3c8Ngfz9qMmIE4nU7y8/NZv349NpuN8ePHs3DhwtNaesC2tYHW/+5DF2ki8uZ0dJEjY4s8b7J+tZqGRx7BdfAgYTfcgPnee9EGnuLUTyGgZhvk/ROK31dXkUyYATnfgclXD3pqZH9dMxdnxHDz7GRmjgmXXTOjmAz1AZSX/4OKyifZXHUbL6V8gxB/Hd8Lr+Uf235OlCmKJ9IfZP1vnybEHHXWZ8QMxOFwsHXrVjZu3EhXVxdpaWksWLCAqKhT+8XTXdZGy5ulCAERyydhSB3ZG26cCndnJ01/eprWN99EFxNDzEMPErRgwemd1N4CO9+GvJfBsg8MoZC9HKZ/G8wTBn2aKouN13q7Zjq61TnvN56XzNVT4wmU672POjLUByCEoKTkARoOvkP+rnt5bcoF2DXw8wQPr2+7F4fbwS+ivs+eF99lXM5MrvjR2Z8RM5Du7m42b97Mpk2bcDgcZGZmMn/+fCIiIoZ8Lpeli+ZXS3A12Qn5xlgC58SNytagvaCAhoceomdfGcGXXkL0Aw8MbvrjQISAyg2Q/08o+UDte0+eq3bNpF0OusHd6dvlcPPhzjpe3VxJUW0Hgf46rpkWz03nJTM+Kuj06iiNGDLUT8LjcbJ9+y20teVTvOWHvDMnl0rFzc+Tglhd9BP2t+3nLvcVdHy6nZlXXcu8ZbcMd5WPY7fb2bhxI1u2bMHtdjN16lTOP//8IS8a5ulx0fKvvXSXWDDlRBN21XgU3bnzS+xsEQ4HzS++iOVvz5/a9MeBdDbBjtchfwW0VqpLEWRep859j80aXP2EuqHH619X8VFhPQ63hznjIrhpdjIXpkejG0WD3qORDPVBcDrb2b79BqzWMvZvuocP51xAntbF9xLCsBz4A2ur13Bj9Qx0RY1ccvd9pM87zT/LzxCr1cqGDRs49O86bdo05s6dS0jI4O9cFB5Bxyp1lUe/5GAibkxDGzQ6F6XqKS+n/qGH1OmPs2YR+8iv8Use+prs/fJ41J2bCl6H3R+B26EOqGbfqA6wDnKTbUtnD//Kq+aNzQeobesiJtjAt2Ykct2MROJDz43uQsm7ZKgPktPZyvaCm7B2lFG14U7WzV7IZ/4uLjeHkNr1P94oeomlO1IJsgiuffDxs7ZGzKlob29n3bp1FBQUoCgKU6dOZd68eUMKd3thE63v7kVj0hFx82T84gc/Pc+XCI+HtnfepfEPf0C43UT9+H7Cli3zbteUvUVdkqDgdajfARo9TLpUDfhxC0F78n5zt0ewencjr26uYv2+JgAumGDm+hlJLEqLQi9b7z5DhvoQHAr2TmsZB9bdSfG0hbwe7GJGcADXBZTwl42PcPmmWILdBr75s1+TMGnycFd5QG1tbWzYsIHt29XNmg+13AfbLeOo7cTyagkeu5OwpRMwTRn5G1ufKmdDA/UP/ALbpk0E5OYS+9hv0MfEeP9CDUWw4w11eqTdAkGx6poz2TdAZOqgTlHdYuedvGreyavmYEcP5iB/lk5P4PoZiSRH+P6Cbr5OhvoQ9Q326nV3Upu2gH/Eeoj11/OzOBt/XXUfczYGENLjz9U/eZiUrKnDXeWTOhTuBQUFCCGYOnUqc+fOJSzs5LNc3J0OLK+V4qjqIGhBIsEXJvvkbkqDIYSg7e23Ofi736PodMQ8+EuCL7/8zAwouxzqvqs73oB9X4Bwq+u+p18Jk68aVMC73B7W7Gni7W3VrN7TiNsjOG9sBNfPTGRJRoy8qWmEkqF+CtRgv5lO6z6q191JR8oFPD9eAwr8bmwAr6z9IRO+6iLcZuCKH/6c1BnnDXeVB6W9vf1wy10IQXZ2NvPmzTtpuAuXh7b/7ce2rQH/1FDCrpkwKtZmPxFHVRV1P/s5XQUFBF14ITG//hW68MH1gZ8Sa4PaPVPyP6jeor4XNVkN90Hu3NTQ3s27edX8K6+amtYuQk16rsqO5+LJMcxICZODqyOIDPVTdDjYO8s4sPZO3NFzeXmqP7UOJ4+Ni2Jr4SOY/ldGZLs/F995L5nnLx7uKg9ae3s7GzduJD8/HyEEU6ZMITc3l8gBpu4JIbBtbaD943JQFEIvH4tpevSonPYIINxuLC+/TPOzf0YTHEzso48QtPAsLCnRXntka74DmwEB5rQjAX+StWc8HvWmpre3HWBVSSMOt4dgg475E6NYlBbF/IlRhBjleu/nMhnqp6FvsFevvwtN4Gz+fUEwW6x2bogNJ8ryLk1vrCTGYmDurd9m9iXXDHeVh6Sjo+NwuLtcLiZNmkRubi6JiYkn/IzL0kXLe/twVLRjmBRO2DfHow0eva327j17qfvpT+nZvZuQq68m+oGfow06S3PGO+qPBHzVJkBA5ESYeAmMnQ9J54H+xEsLdPa4WL+3iVWljaze00iLzYFOozAjJZxFaVEsTosmJVL2wZ9rZKifpkPBbusso3rDXeg0M9l3TSzPN1iYHGjgW8Z97H7hGeIbDUz+5hUs+dYdw13lIevs7GTr1q1s27aNrq4uEhMTyc3NZcKECf1u1iE8gs6v6+j4tBK0GkKvHIcp2zx6W+0OB03PPYfl7/9AFxNN3OOPEzB79tmthLXhyN6rBzarNzjpDJA0G8YuUEM+JuuEG3y4PYId1a2sKm3ky9KD7D2o7nkzzhzA4rRo5qWamZoUSoC8g3XYyVD3gr7BXrPxLoR9KgE3juWh1mZ6PIIfRLs48PcHiKvREbv4PJbd/sCIDDiHw0FBQQGbNm2ivb2dyMhI5syZQ1ZWVr97qDqb7LS+tw9HVQeG9AjCrh4/aue0A3Tt3EndT3+Go7KSkCuvIOr++9GZh2HGUE+n2nIvXw3la6CxRH3fFAFjLlADftwCCD3xgnAHLHa+3H2QL0sb2VJhwekWaDUKGXHBzEgJZ8aYcGakhMuNtYeBDHUvORzstjKaCu6hZX8aqUvH8kxwD9s67FxrNuL/9i+I3deN/8xx3PnDP3l9v9Gzxe12U1JSwsaNG2loaCAwMJDZs2czffp0jMajb2gRHkHnhlraP69E46cl9KrxmLJG79RHT1cXzc+/QMvLL6P4+2P+wT2ELV+OcgY2Fh80awOUrz0S8tZ69f2wMZA4U11wLH66evOT9vj+9M4eF/lVrWytsLCtopUdNW04XB4AxkcFMnNMODN7g17e8HTmyVD3IqezlYKCW7B2luBs+hb7Vy9gTE40W+aF8UJdM5MD/Mle/zfitpbTnR7Gjx54AaN+gM2Oz3FCCMrLy9m4cSPl5eX4+fkxdepUpk+fftziYc5GOy3v7MFZ04kxM5LQq8ajDRi9A249FRUcfOxxbBs24D9hAjEP/hLTjBnDXS11HZqmPWq4V65XV5TsPKge0xkgNhsScnofMyA4Ho75q7PH5aawpp2tFS1sq2whv7IVa48LgLgQAxnxIWTGh5DR+zAHjd4xlzNBhrqXud12du95kIaG/6LzzGD3h8sICovGsHwMDxxsxCUEF5WvZtzKL2hJ1vHt+3/H+KjBr8h3rqqvr2fTpk0UFxfj8XhISkpi+vTppKeno9er4S3cAuu6ajpWHUBj0BF8YRIBM2JG1QYcfQkhsK5axcEnnsBVV0/wFZcTdf/96E9xNc0zQghor4HaPKjJU0O+bge4e9TjQbFqKz4uW+2Tj86A4Lijgt7tEexu6GBbRQvbD7RRVNtOebPt8PHoYH8y40OYHHck7KOD/UdkF+W5wFt7lC4BnkHdo/RFIcSTxxz/EXA74AKagO8IIaoGOudIDXVQ/2etrXuLvXsfRasJo3r997A1JpO+PJUn9TbyO+zkWquY+dbL2Iw9xF23kNsu+iF6zchvudpsNnbs2EF+fj4tLS0YDAays7OZPn065t7+Y2eDjdb/luGo7EBnNhKyZAyG9NG7Brinq4vmF16g5aWXUfz81C6ZG24Y3i6ZgbgccLBIDfna3qBvKT9y3BimdtVEZ0JMhvp15MSj9mm1djspqetgV207xb3P+5s6ORQ3kYH+pMcFkxYbRHpsMJNighlrDpDLGQzCaYe6oihaYC9wIVADbAOWCSFK+pRZAGwRQtgVRfk+MF8I8a2BzjuSQ/2Qjo5CdhXdTU/PQexVN1O1eTaTLohn7bRA/l7XzHiNi1kfvkhMdTV1GXq+c8evyYyeMtzV9gqPx0NlZSX5+fmUlpbi8XhITk4+3HrXarV0l7TQ/mkFrqYu/FKCCbl0DP5JwcNd9WHjqKyk4fHHsa1bj39qKjEPPXhudMkMRncHHCxWw75hl/poLFE3AQF1vRrzRLUlHzVJnTsfNQlCkg7PuLH1uCitVwN+V207pfVWyhqtON1qBvlpNaRGB5IWG9z7UAM/1CQHY/vyRqifB/xKCHFx7+ufAwghnjhB+anAX4QQuQOd1xdCHcDpbKO45H4sltVoHfMp+egazPFmtN9K5sG6RlqdLmY27mXGR+9gN3QQ/s1c7rzwJxh1vjOg1NnZebj13traitFoJDs7m8zMTGKiorHnNdKxqgpPpxNjViQhF6egi/Cd738ohBB0fvUVBx97HGddHUEXXkjkXXdimDRpuKs2dB43WPZDQ2Fv2Bepz4cGYgH0JoicAOZJR8LePBFCk0Gjwen2sL+pk9L6DkrrrYefmzt7Dp8iOtif1KggxkcFMi4qkPHmQMZFBWAOHJ1dON4I9aXAEiHE7b2vbwJmCSHuPkH5vwANQojf9HPsDuAOgKSkpOlVVQP20IwYQnioqnqe/eV/Qq9JonzV7bjs8cy6ZRJvGR2sqG0mGA+zNn5AenEeFZM93PLtX3Je/JzhrrpXeTweKioqyM/PZ/fu3Xg8HsLDw8nIyGDyxDQMxT10rq9FeASBs2MJWpg0agdTPV1dWF56mZYVK/B0dhK4eBHmO+/EkJ4+3FU7fV1t6mBs02710ViqvrbWHSmjN0HEOIhIVUM/MhUixqsP/0CarD3sbuigtL6D3Q1W9jd2UtbYic3hPnyKEKOeceYAxkcFHn6MjQwkPszo09043gj1a4GLjwn1mUKIe/opeyNwN3CBEKLn2ON9+UpLva+Wlo0UFd+L291FW+nt1O3MYtLsGEKWJPDr2oNs67CRZG1i/mfvgLsC0+XT+OHiXxDiP/glcUcKu91OaWkpRUVFVFZWIoQgKiqKyalpJDWFoN9lQ/HXErwgUd1lST86F5dyd3TQ8uprtLz6Kp6ODgIXLCDyzjsxZmYMd9W8r6sNmvf2hvxu9evmfdB2AOiTQ0FxEDm+N/BT1efwMYiQBBpsHsp6A/7QY39TJ82djsMf12oUEsKMJEcEMCbCRHJEACmR6nNimAm/Eb7xy1nrflEUZTHwZ9RAbzxZxXwx1AG6exooKvoB7e356HquoHTlxeh0JmZcPobSiSZ+U15Pi9NJVuk2ztv6KeUTOli+7MdclHKRz/4pabVaKSkpoaioiOrqagBizTGMdUWRVB9EUEAgATNjCZwdizZkdE5/c1uttL7+OpYVr+BpbyfggvMx33UXxqzB7YY0ojm71YFYyz415C1l6nPzPuhp71NQgZAECEuBsOTe5zEQNoY2/zjKOv2osNipstipsNiostiobLbT2TvdEkCjQHyYkZSIABLDTSSHm0gKN5EUoT4HGc79vxy9Eeo61IHSRUAt6kDpciFEcZ8yU4H3ULtp9g2mYr4a6qBukbd//+85UP0Sel0MHWU3ULV1IuakYKZ+azyveGy8XNOM0dHNvI2fEGbZRM/CJO6Y9wOyo7KHu/pnVFtbG8XFxRQVFVFfr/a9xvhHkGgLJcljJjYjmaDcePySgnz2l9xA3J2dtL7+Bi3//Cfu9nYC5s0j8s7vY5p67i/x7HVCgK1JDfnWKnX7v9ZKaK1Qnw/Nr///7Z15kB3Hfd8/PTNv3rHv7dv7BrALQuIBkqB4HwotkTJtq2KJdkRJdiXlVLlK+iNyOZV/lMofsayUq+xUlNi57FJipSwnoqyDcliJU5ZE0pQYpSRCIiiSAAkCWBy72AN7vPu9Obp/+WNmDwALYAkei/cwn6pf9TEHutH7vt3z657pNdxCJPY9u9dNirsopcc4pQc4XnU4tVyPhb/O6ZUGpUZw3i36utyLxH6iN8tET47Rnsw14dZ5p5Y0fhT4Y6IljV8RkT9QSn0ROCgiTyulvg/cBqzNkJwWkY9d7p6dLOprlEoHeePoF6jVjpC272b6+V+nMjfI/g+O0f3YOL93eo4XKw1GF2f4hR/9L2rpw6QfvJHPPPQ5Dgx2xiqZy7G0tMRrr73G66+/vi7w3WTZHQ6wt28X+37hNgoHhq/LfVJ1rc7qk19j5Sv/Db26Su6B++n9zd+k8KEPoVLX/mjyPcGvR66bNbFfmY7SpdNQOgV+7fzz08XzIpQvZQAAGItJREFUBJ/iBPXsCPMMcDLo5Wg9x+nVFqdXIsE/W2qhzYY+WgqGuzOM92QZ782uhxO9uSjekyXrvvtuxOTlox1GRDM7+yTHT3wJrRtYjY/x+t9+mJRb4P7H9/Lq3gz/6vgsS6Fh4uw097z8A1rqp+QeupHPPHB9iDtEnwM+evQob7z+BtPT02ijccVhtzXIje9/P7f84l10DVx/yyFNo8Hqk19n5atfJVxYwB4coOfxX6PniU/g7r70t1uue0SguRqJ+7rQX2AXir6VguI4FHdBcQJdGGPVGWZB9TMT9nLM6+Z41WG21GK21GSufL7oQzTSH+/JMtaTYSwW+rHYxnuy9He5WG9zk5lE1K8RfH+Z4ye+xNmz38Cx+6gc+zSnX7ydkb293P2pfXzX8vnPJ+eYCw2Dy/Pc/fLzaP0jCg+8n8/e/zvcPngd+FZjPM/j+PHjHDn4Cm+ePE7L+ChRjHcNsnffDey78ybGJsa3/MhYpyJhSO2HP6T0jW9Se/55MCYavX/yk+QffRTLTdZyvyVEoFWO3qYtz0D5DFRmN6VnoHI22nFqM6lc9EZt9ximMEY9Pcyy1c+c9HI66OVYq8CbtTSzZZ/ZUpOGf/71rmMxVszwjx6Y5Lc/OHVVRU9E/RqjUvk5bxz9fSqVQ6TUbZx64RNUz45x04Oj3PLoLp5XPv/+xCzHfU2xssqdr/wQq/V39Ny/j8/e97nrStwhfsnpteMcfuEQJxZOsUI0unKUza6RcaZu2sfU3inGxsaw7etjBU0wP0/pqacof+vbBGfPYvf2Unz8cXqeeIL03qsTioQt0CHU5iNxr8zG4QXx6hyY8PzrlA35IaQwQpgbppoaYMXqZ1F6mAmLHG8VuG3/fv7+fVe3x3Ei6tcgIoa5uac4dvyPCIJVrOavcOzZx/DrXey5tZ/bH53g8IDNHx+b4VArJNusc8drPyJd+y5990zxD27/FA9PPNwRnx14K0hoWHn1LMdffJ1TM6c5KyusWpHIp5wUu/fsZnJyksnJyetC5EVr6j/6EaVvfJPqc89BGJK7+266P/arFB55BOcyO1klvEMYHU3mrgl9dX7DamvxuWgT8c08+Dvw2EWv8myLRNSvYYKgwvT0nzAz+5eAhR38PWYO3kfp9BQDuwoceHQXK+/P8+/ePMUPGyGpwOPW1w9SXHkBb2CBB+79JT6+73Fu7LvyHpWdhgSa1hurLP/sDCffnOasrDDnlFhdG8k7DmNjY0xMTKxbd3fn+uTDc+cofeevKX37WwSnToNSZO+4g8JHHqXw6KO4k5M7XcTrm9CLVutUFyKR752E0at76k5EvQ2o148zM/vfmZ//DmFYxWYPq8ceZv6Vu8jl+7jtwxNYd/bzH06c4v/UPIyy6Ftd5IaTL5Op/z9yeyx+6Z5f56NTH6Un07PT1XnPMb6m9foKzZfPsfLGAvNmmcV0jaV0jUV/FW0iv2Z3d/d5Ij86Orr+hclOQUTwjh6l+v3vU33mGbzDRwBw991A4dGPUPjIo2T270e16bf+ExJRbyu0brKw8L+ZPfsklcohFGn8lfuZ/dkD6Pr7uOWhcSYeHuFZr8ZfTc/yc2MjSjF8bpY9p1/C9X/C1G17+NUPPMGDYw/iWNfPROIaxgtpHVmhdXSV1pslgmqLZVVludDkXK7OQrBCuV4BwLIshoeHGR0dZWRkhNHRUYaGhkinO+cFqGB2luozz1J95hkaBw+C1jjDw+Qf+TCFRx4hd9ddWLn2/eb/9Ugi6m1Ktfoas7NPMr/wNFrXEX+SxVcfonzqPgYnhpk6MEBufw/fa1X45ul53rSi1Q8TZ08yNvczuvg5++88wAP7HubekXspuO/RZsjXECJCuNigdayE92YJ70QZ8TUN5bEyGLBcaLCoSyyWlmi2muvX9ff3r4v8yMgIIyMj5PP5HazJO4Mulag9/zzV7z9D7YUXkGYTHIfs/v3k7r2X3L33kP3Andj5ZLPpa5lE1NucMKwxv/A0s7NPUqsdBknjl25l+fgt1M/eTr44xtTtA1i3FHk2LPHU/DIzTgZlNOPzZxhcehPXO8JQb5PbDtzHg7s/yIHBA6S22Las0xFt8M9Uab1ZwjtWwj9TAQOihNaAotwbsOLWWQrLLKyeo1zeeEU9n88zNDTE4OAgAwMD65bP59vyzVfTatF48SCNF1+k8ZOf0Hz1VQhDsG0y+/fTde895O65h+xdd2F3QIfWSSSi3iGICJXqz5mf+w5Ly8/Sas0CoJt7KU3vpzp7Gyrcx57bBjH7izxnSjxfqnDCTiPKwgkDRhdO07d6lHRwlMlRh7tufYgHJh7kfT3va0theruYVoh3qkJwpoofm2lEy9NUykKPuhtCH5RYKq+wtLREEGy8Wp5Op88T+YGBAfr7++nt7W0rf71pNGi89FIs8i/SfOUVCAKwLDK33EL2wAEyt9xM5uabSe/bh0rWxe8Yiah3ICJCvX6UpaXnWFp+lnL5JcAgYS/V2VupnLkNb2U/I3tHyE8VOD1s8dPWMj9uepx2o0frVOAzunCSYvkNcjLN1JDLTe87wG3Dt3PrwK0d+eXIKyEi6JXWusD7Z6r4Z2sQRr8RlXFwhrJ4fYpK1qNsN1gNKixXVllaWqJarZ53v3w+T29vL729vfT09JwX7+7uvqY3JjfNJs1Dh9ZFvnX4MKbRiA6mUqT37SNzcyTymVtuJn3jTYnb5j0iEfXrAN9fYXn5eZaWn2V5+QdoXQNx8Cv7qM3tobk8RXNlL4XiOLkbCpweFg5JmZeMYS4T+dotHTK4skBP6TTp1jTdzhI3jHVzyw13cPvwAW7qu6mjNvbYLhIagvk6/pkqwXydYKFBuNhYH9EDqKxDaiiHGUhRzfmUU02qpkG5VWV1dZVSqUSlUmHzb82yLHp6eigWi5e0a2mkL8bgnzqFd+QIrSNHaB2OQr2yEp2gFO7u3aRvvhl3apL01BTu1BTu5CR24fqbz3k3SUT9OsOYgFL5IMtLz1EqH6RaPYxI5C6QoJ/G8iT1hUmay3vRjb2kJgeYH9OcSDU4ahmm0114qejR2gl8BpfnKFZOk/ZO0u+W2TtWZM/uG7mhbx9TxSl2FXZdd6tsRARTCyKBX6gTLDa2FHtshdOXwenPonpTNPOGqtOiSpOyV6NULlEulymXyxeN8gFyudy6wHd3d1MoFMjn8xQKhfV4LpfbMddZNBG9SOvwYbzXX4+E/ugbBDOzoDdej7cHBkhPTuJOTeJOboi9OzGeuHGugkTUr3O09qjVDlOuHKJSeZly+RCtVvRNc8QibOyivrCbVmUEvzqCVxuh0T/BuWHhVJfPdFpxJlcgcKJRox0G9FRWKNTOkWkukAoWKKYajPc4TI1PsG/8JqaKU0x1T5FLXV9L5dbFfrGBXm4RLDfRS03C5RbhchMJzMbJViT4dk8au9uFgkMjHdCwfWrSohrWqTRrVCqVdeH3vIv3nbEsa13o18Kuri5yudyW9l6M/sX38Wdm8Ken8aen8U6exJ8+iT89vTGyB1AKZ3iY1Pg4qfExUuPjuBMTcXqc1MhI8kXKLUhEPeEifH85EvjKISrll6lUXyUMS+vHxTjo5hDN0hB+ZYRmdZRZdjPf3c+5fIrFrGIx7bKcy2M2vYqf9pr0lJfoaizieotkTJmeVMhQ3mbPYA+7RqeY6N3NRH6C0a7R62oFjohgqgHhcjOypUjoddmLrOqDueAiW2F3u9jdaeyii8lZNN2QpuPTUD4N41EPm9S9BrV6jWq1Sq1Wo7Hm+96CVCp1WdG/0LLZ7Dv6uQVdLuOfPIk3PU0wM0swO0swMxOF8/NgNnd8Fs7IMKmxMZzBwS0tNTSEVSxeVxP9iagnbAvfX6HROEGjMU2jcYJ64wSN+gmazdMIGy4FE+YI6n0E9T5a9QEWwwnmzABL6R7OpfMsZjIsdWUp5wqw+YcmhnyjRr62Sra5guuvkjMVirZPf1YxmM8y2tPD+NA4Q33jDOWGGMwN0pW6PibfxAim5qPLfiTylS3Cqo94+uKLFVi5FHYhhVVwUTkbPy14KY1nh7SsgBY+LePT1B5Nv0Wj2aDR2DDf9y++b0wmkyGbzZLJZC5r2WyWdDp9kW23U5AgIFhY3BD5NTt7lvDcOcJz5zD1+sXVd12cgQGcwUHsgQGcvl7svv710O7rxenvx+7tw+ntaXuXTyLqCW8LY0JarZl1sW+2ztBqztJozNLyzmLM+b5gMTZBo49GY5Bz4ThLeoAleli2iqw6BVbcPKvpPJVMHm1f7ItPe01yjRoZr4brV3HDGlnTJKt8Co7Q7Sp6sykGunKMFosM9Q3SXxymN9NLMV2k4Baw1LW7quTtYnyNqQXoqo+p+uiaj64GUYdQjfPrAaYebN0BxKiMg9XlYGUcrKyDcVnvBDwrpKl8WhLQEo9m6OFpHy/0afkenu/R8lq0Wq3zlndeCsdxSKfTuK67pehfaFudl0qlSKVSKM9DLy0RLi6uC/15trxCuLKMXlk9z6+/Gau7G7u3B7u7iN3djV3sxip0nx8vRmmru4hdyGPl81iFwjXxieNE1BPeVcKwSqt1dpPN0mzO0KjP4fvLhOEKhosnAQ2Kku5nIdjNajhIyfRSNj1UKFBReapWF1W7i1qqi4Z76dF6KvDJtBqk/QZpv04qqOOGTVzxcAlIW5qsJeRSioLrUMyk6M1lGcjnGeruo7fQRyFXJJfKkXNydKW6SNvpjnicl9BgGgG6HkZC3wjWBd80QnQ9QFohpqUxzTCOh4h/oR9oCxwLK2MjaYvANQQpg+8YAjvEtw2B0gQqJCDEl5BAQnwT4OmAQAd4oY8f+Hi+j+d76EsI8IUopXBdd13kU6nUJdOOMdhhiOP7WK0WdrOJVa9j1WpY1RqqFplVqUClAqUStu9ja82lWl+5bizweeyuWOjX4vk8Vi6Hlcti5XKoXA4rmzsvz8rGYbHnqpeAJqKesOMY4+MHKwT+Mv6aBct43hLN+jl8bxXfXyXUZbSpIFIBtfEj11hU6aZCkSoFaqabiumhaopUTXeUR56a6qKuuqjbXTTtLEZd4bFfDG7g4wY+qcDDCX1SoY+tfRzt45gARwJSEpAixFUG1xJcGzK2IuNYZFMOXW6KLjdFdyZDdyZLMddFd7aLfLqLXCZHJpUl7aRJ25G5tntNP02INpiWRpqRyJtmiHg6En8vRFoa42nEizqE6NhaftQpiK/Pnxi+AhpDYGuClCF0hMAxhLbGtwyBFRIqg1ZRZ6ExBGhCNKFoQgkJRBOakECHBHEY6gBttl+GzdiWhWPbOEphA7ZIZMZgG4MVhpEFAZYfYPkeludHYRBgGYOlTRQag230eto2monHHuOmz3/+qsp2JVHf1lo0pdQvA39CtEfpfxWRP7zgeBr4KnAXsAx8SkROXlWJEzoOy3LJpEfIpEe2db6IoHWNICgThCXCoEwQrEbpoIrfquB5VYKgSujPEeoaWtfRpoZIHVF1BA9P0jTJ0YitGdtGuotmKksrlaElGVpk4zBDizweGVoqTUtlMGobP5UQqMUGKDE4poSjz2EbjW1CHB1EcR1i6xDL6OgHbzS2GGytscRgY7DF4CjBIQ6V4Fjg2OBYipSlSDkWrm2Rsm1c28J1bNK2Q9pxSDspMmthyiWbcsm6aXKuS1cqS851cd0MjuXgWA4pK4Vt2TjKwe5KQdfbm8gWI5G4+3EnEIu98aOOQHyDhHEYGCTQcbjRKZi1/FAg0EggcdpE8VBfPMG8CYMhxBCiCVTcEWAI485Bx2mtDKElaDuKa2XQIoRojJKNPNtEnYobhSEaI4ZQNFoMWjTaGITLD5bvGRjnprf1v3tprviXqpSygf8E/CIwA7yolHpaRA5vOu23gVUR2aeU+jTwR8Cn3o0CJ3Q+Sikcp4DjFMgycVX3EDFo3USbJjqsR6GuR3m6jg4bBEED36sRBk3CoEYYnkOHXnRO2MKYFtp4iPHwJMQTwQNagI/CVxaesvEtB580Hml8XDwyBDiEpAgsl8BKEXChuevxEBcvzgtx4rzomN5OZ7IVOraLV0ESqWAdqKPEYIvGEh2HBot4ZImJ0mv5m0yJwRLZFI/SCtmIrx0njq+FIlgQ5SNYAipOW4CFAApLgeWClRYUKrpGgY1CqShtKYWtFI5Y2Ng4ysIWC5vIHOz1tIOFLQpHbBxcbFHxeQrHKBxjYQG2KNIS5dmisI1CGbANWKKwDFhGYRtQhiitBUuDErBEMEZQxmAwiBgimTcIBiOGvt7xq2vXbbCdv5h7gWMicgJAKfV14OPAZlH/OPCFOP4t4D8qpZTslG8n4bpHKQvH6cKhC9x3dwcgEUHExxgfY7w49DESIMbHmACjfcLQIwx9dBCHoYfRPtrUMDrA6ABtolBMFNc6wBPBNyG+MQTGEEhkoZEojhBIpOEhEKIIlUKzYaGy0UqhsdDKQisbrSxMnDbYaMuO01EY4mCwCbERLDQ2sdRH52PHcQuf1EV5a3Z+XvxvbDouV3KTdSCPLP1fvsYd78q9tyPq48CZTekZ4L5LnSMioVKqDPQDS5tPUkp9BvgMwO5kJ/SEDkEphVJpLCsNdMYr8VFHFQ33RQwiURilN/KMaMRojNGIDtFGI8ZEaROdZ9aOG40xIWLi+xmzfh9thNBoAmPQJkQbQ2g02mhCbaK0aML4eCgSuTu0id0ehlBM5LqTeEwsBiNEcbM2Tpb1PIE4zkZcEY2pRSEqyhOInilUfA0KhOh4fIw4X+J7oTbSrF2/KXy/m3nX2m47or7VRPCFI/DtnIOIfBn4MkQTpdv4txMSEnaAqKNy2Oa0W8I1xHam4GeAXZvSE8DZS52jor+EIrBCQkJCQsJ7ynZE/UXgfUqpKaWUC3waePqCc54GfiuOfwJ4NvGnJyQkJLz3XPHZKvaRfw74W6IljV8RkdeUUl8EDorI08CfA3+plDpGNEL/9LtZ6ISEhISErdmWw0xE/gb4mwvy/uWmeAt44p0tWkJCQkLCW+Xafa0tISEhIeEtk4h6QkJCQgeRiHpCQkJCB5GIekJCQkIHsWNfaVRKnQNOXeXlA1zwtmoH0Gl16rT6QOfVqdPqA51Xp63qs0dEBi91wY6J+ttBKXXwcp+ebEc6rU6dVh/ovDp1Wn2g8+p0NfVJ3C8JCQkJHUQi6gkJCQkdRLuK+pd3ugDvAp1Wp06rD3RenTqtPtB5dXrL9WlLn3pCQkJCwta060g9ISEhIWELElFPSEhI6CDaTtSVUr+slHpDKXVMKfXPd7o87wRKqZNKqVeUUoeUUgd3ujxvFaXUV5RSi0qpVzfl9SmlvqeUejMOe3eyjG+VS9TpC0qp2bidDimlPrqTZXwrKKV2KaWeU0odUUq9ppT63Ti/LdvpMvVp5zbKKKV+opR6Oa7T78f5U0qpH8dt9FfxJ9AvfZ928qnHm2AfZdMm2MBvXLAJdtuhlDoJ3C0ibfnShFLqYaAGfFVEbo3z/jWwIiJ/GHe+vSLy+Z0s51vhEnX6AlATkX+zk2W7GpRSo8CoiPxMKVUAfgo8Dvxj2rCdLlOfT9K+baSALhGpKaVSwAvA7wL/DHhKRL6ulPoz4GUR+dNL3afdRurrm2CLiA+sbYKdsIOIyA+4eKerjwN/Ecf/gugH1zZcok5ti4jMicjP4ngVOEK0t3BbttNl6tO2SEQtTqZiE+AR4Ftx/hXbqN1EfatNsNu6IWME+K5S6qfx5tydwLCIzEH0AwSGdrg87xSfU0r9PHbPtIWr4kKUUpPAB4Af0wHtdEF9oI3bSCllK6UOAYvA94DjQElEwviUK2peu4n6tja4bkMeEpE7gV8B/kn86J9w7fGnwA3AHcAc8KWdLc5bRymVB74N/FMRqex0ed4uW9SnrdtIRLSI3EG0F/S9wM1bnXa5e7SbqG9nE+y2Q0TOxuEi8B2ixmx3FmK/55r/c3GHy/O2EZGF+EdngP9Cm7VT7Kf9NvA/ROSpOLtt22mr+rR7G60hIiXg74D7gR6l1NoudVfUvHYT9e1sgt1WKKW64okelFJdwGPAq5e/qi3YvBn5bwH/cwfL8o6wJn4xv0YbtVM8CffnwBER+bebDrVlO12qPm3eRoNKqZ44ngU+QjRX8Bzwifi0K7ZRW61+AYiXKP0xG5tg/8EOF+ltoZTaSzQ6h2jP2K+1W52UUk8CHyL6TOgC8HvAXwPfAHYDp4EnRKRtJh4vUacPET3WC3AS+OyaP/paRyn1QeCHwCuAibP/BZEfuu3a6TL1+Q3at41uJ5oItYkG3N8QkS/GGvF1oA94CfiHIuJd8j7tJuoJCQkJCZem3dwvCQkJCQmXIRH1hISEhA4iEfWEhISEDiIR9YSEhIQOIhH1hISEhA4iEfWEhISEDiIR9YSEhIQO4v8D6T3ToqgKJtQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iterating to find the best hidden parameter, train and test the model to find the best accuracies:\n",
    "\n",
    "n_hidden=(i for i in range(1,11))\n",
    "storage=dict()\n",
    "\n",
    "for i in n_hidden:\n",
    "    net=One_hidden(i)\n",
    "    storage[i]=model_train(net,trainloader)\n",
    "    \n",
    "print(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.49642\n",
      "Epoch [2/20], Loss: 0.06794\n",
      "Epoch [3/20], Loss: 0.07186\n",
      "Epoch [4/20], Loss: 0.23103\n",
      "Epoch [5/20], Loss: 0.10879\n",
      "Epoch [6/20], Loss: 0.00257\n",
      "Epoch [7/20], Loss: 0.00310\n",
      "Epoch [8/20], Loss: 0.03676\n",
      "Epoch [9/20], Loss: 0.00180\n",
      "Epoch [10/20], Loss: 0.01946\n",
      "Epoch [11/20], Loss: 0.00233\n",
      "Epoch [12/20], Loss: 0.01456\n",
      "Epoch [13/20], Loss: 0.00029\n",
      "Epoch [14/20], Loss: 0.03122\n",
      "Epoch [15/20], Loss: 0.00873\n",
      "Epoch [16/20], Loss: 0.00411\n",
      "Epoch [17/20], Loss: 0.01046\n",
      "Epoch [18/20], Loss: 0.00026\n",
      "Epoch [19/20], Loss: 0.00102\n",
      "Epoch [20/20], Loss: 0.00420\n",
      "Accuracy of the network for val data: 100.0 %\n",
      "Training time: 1.3472719192504883s\n"
     ]
    }
   ],
   "source": [
    "#Better to take the largest no. of hidden nodes for good computation:\n",
    "start_time_best=time.time()\n",
    "model_best=One_hidden(10) # best parameter\n",
    "model_train(model_best,mainloader)\n",
    "end_time_best=time.time()\n",
    "print(f\"Training time: {end_time_best-start_time_best}s\") # training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network for val data: 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(net_besT,testloader) #final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
